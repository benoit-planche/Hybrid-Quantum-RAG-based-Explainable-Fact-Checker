# ğŸŒ RAG-based Explainable Fact Checker

Un systÃ¨me de fact-checking basÃ© sur RAG (Retrieval-Augmented Generation) pour dÃ©tecter la dÃ©sinformation sur le changement climatique.

## ğŸ¯ FonctionnalitÃ©s

- **Fact-checking automatique** sur le changement climatique
- **Recherche dans une base de connaissances** scientifique
- **RÃ©ponses explicables** avec sources
- **Interface Streamlit** intuitive
- **Ã‰valuation complÃ¨te** avec DeepEval

## ğŸ—ï¸ Architecture

```
â”œâ”€â”€ app.py                    # Application principale Streamlit
â”œâ”€â”€ ollama_utils.py           # Utilitaires Ollama (LLM + Embeddings)
â”œâ”€â”€ ollama_config.py          # Configuration Ollama
â”œâ”€â”€ data_loader_ollama.py     # Chargeur de donnÃ©es avec Pinecone
â”œâ”€â”€ pdf_loader.py             # Chargeur de documents PDF
â”œâ”€â”€ evaluate_with_dataset.py  # Script d'Ã©valuation DeepEval
â”œâ”€â”€ climate_dataset.py        # Dataset d'Ã©valuation (100+ questions)
â””â”€â”€ requirements.txt          # DÃ©pendances principales
```

## ğŸš€ Installation

### 1. Cloner le repository

```bash
git clone <repository-url>
cd RAG-based-Explainable-Fact-Checker
```

### 2. Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```

### 3. Configuration Ollama

```bash
# Installer Ollama
curl -fsSL https://ollama.ai/install.sh | sh

# DÃ©marrer Ollama
ollama serve

# TÃ©lÃ©charger le modÃ¨le
ollama pull llama2:7b
```

### 4. Configuration Pinecone

1. CrÃ©er un compte sur [Pinecone](https://www.pinecone.io/)
2. CrÃ©er un index Serverless
3. Copier l'API key

### 5. Configuration environnement

```bash
cp env_example.txt .env
# Ã‰diter .env avec vos clÃ©s API
```

## ğŸ”§ Configuration

### Variables d'environnement (.env)

```env
# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2:7b
OLLAMA_TEMPERATURE=0.7
OLLAMA_MAX_TOKENS=2000

# Pinecone Serverless Configuration
PINECONE_API_KEY=votre_cle_api_pinecone
PINECONE_INDEX_NAME=fact-checker-index
```

## ğŸ“Š Chargement des donnÃ©es

### 1. PrÃ©parer les documents

Placez vos fichiers PDF dans le dossier `rapport/`

### 2. Charger vers Pinecone

```bash
python data_loader_ollama.py
```

## ğŸ® Utilisation

### Lancer l'application

```bash
streamlit run app.py
```

### Utiliser l'interface

1. **Posez une question** sur le changement climatique
2. **Obtenez une rÃ©ponse** fact-checkÃ©e
3. **Consultez les sources** utilisÃ©es
4. **Explorez les explications** dÃ©taillÃ©es

## ğŸ§ª Ã‰valuation

### Installation DeepEval

```bash
pip install -r requirements_eval.txt
```

### Ã‰valuation complÃ¨te

```bash
python evaluate_with_dataset.py
```

### MÃ©triques Ã©valuÃ©es

- **AnswerRelevancy** : Pertinence de la rÃ©ponse
- **ContextRelevancy** : Pertinence du contexte
- **Faithfulness** : FidÃ©litÃ© aux sources

## ğŸ“ Structure des fichiers

### Core du systÃ¨me

- `app.py` - Application Streamlit principale
- `ollama_utils.py` - Utilitaires Ollama (LLM + Embeddings)
- `ollama_config.py` - Configuration Ollama
- `data_loader_ollama.py` - Chargeur de donnÃ©es avec Pinecone
- `pdf_loader.py` - Chargeur de documents PDF

### Ã‰valuation

- `evaluate_with_dataset.py` - Script d'Ã©valuation DeepEval
- `climate_dataset.py` - Dataset d'Ã©valuation (100+ questions)
- `requirements_eval.txt` - DÃ©pendances Ã©valuation
- `README_EVALUATION.md` - Guide d'Ã©valuation dÃ©taillÃ©

### Configuration

- `requirements.txt` - DÃ©pendances principales
- `env_example.txt` - Exemple de configuration
- `.gitignore` - Fichiers Ã  ignorer

## ğŸ” FonctionnalitÃ©s avancÃ©es

### Recherche RAG

- **Embeddings** avec Ollama
- **Stockage vectoriel** Pinecone Serverless
- **Recherche sÃ©mantique** pour contexte pertinent

### Interface utilisateur

- **Interface Streamlit** moderne
- **RÃ©ponses explicables** avec sources
- **Historique des questions**
- **Export des rÃ©sultats**

### Ã‰valuation automatique

- **Dataset de 100+ questions** sur le climat
- **MÃ©triques DeepEval** standardisÃ©es
- **Comparaison RAG vs Direct**
- **Analyse par catÃ©gorie**

## ğŸš¨ DÃ©pannage

### Ollama ne rÃ©pond pas

```bash
# VÃ©rifier qu'Ollama est dÃ©marrÃ©
ollama serve

# Tester la connexion
curl http://localhost:11434/api/tags
```

### Pinecone erreur de connexion

- VÃ©rifiez votre API key
- VÃ©rifiez que l'index existe
- Testez avec `python test_pinecone_serverless.py`

### Erreur de dÃ©pendances

```bash
pip install --upgrade -r requirements.txt
```

## ğŸ“ˆ Performance

### Temps de traitement

- **Embeddings** : ~30 secondes par chunk
- **GÃ©nÃ©ration de rÃ©ponse** : ~10-20 secondes
- **Recherche RAG** : ~5-10 secondes

### Optimisations possibles

- Utiliser un modÃ¨le plus rapide pour les embeddings
- ParallÃ©liser les requÃªtes
- Optimiser la taille des chunks

## ğŸ¤ Contribution

1. Fork le projet
2. CrÃ©er une branche feature
3. Commit vos changements
4. Push vers la branche
5. Ouvrir une Pull Request

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir `LICENSE` pour plus de dÃ©tails.

## ğŸ“ Support

Pour toute question ou problÃ¨me :

1. VÃ©rifiez la documentation
2. Consultez les logs d'erreur
3. Testez chaque composant individuellement
4. Ouvrez une issue sur GitHub
