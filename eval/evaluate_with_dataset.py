#!/usr/bin/env python3
"""
√âvaluation du fact-checker avec le dataset complet
"""

import os
import asyncio
import json
from dotenv import load_dotenv
from deepeval import evaluate
from deepeval.metrics import AnswerRelevancy, ContextRelevancy, Faithfulness
from deepeval.test_case import LLMTestCase
from ollama_utils import OllamaClient, OllamaEmbeddings
from ollama_config import config
from pinecone import Pinecone
from climate_dataset import CLIMATE_DATASET, get_dataset_by_category, get_random_subset

# Load environment variables
load_dotenv()

class ComprehensiveFactCheckerEvaluator:
    """√âvaluateur complet pour le syst√®me de fact-checking"""
    
    def __init__(self, use_rag=True):
        self.ollama_client = OllamaClient()
        self.use_rag = use_rag
        
        if use_rag:
            self.embeddings = OllamaEmbeddings()
            # Initialize Pinecone
            pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
            self.index = pc.Index(os.getenv("PINECONE_INDEX_NAME"))
    
    async def get_model_response(self, question: str) -> str:
        """Obtenir la r√©ponse du mod√®le pour une question donn√©e"""
        
        if self.use_rag:
            return await self.get_rag_response(question)
        else:
            return await self.get_direct_response(question)
    
    async def get_direct_response(self, question: str) -> str:
        """Obtenir une r√©ponse directe sans RAG"""
        
        prompt = f"""
        Tu es un expert en fact-checking sur le changement climatique. 
        R√©ponds √† la question suivante de mani√®re factuelle et pr√©cise, 
        en te basant sur les connaissances scientifiques actuelles.
        
        Question: {question}
        
        R√©ponse:
        """
        
        try:
            response = await self.ollama_client.generate_response(prompt)
            return response
        except Exception as e:
            print(f"Erreur lors de la g√©n√©ration de r√©ponse: {e}")
            return "Erreur lors de la g√©n√©ration de r√©ponse"
    
    async def get_rag_response(self, question: str) -> str:
        """Obtenir une r√©ponse RAG pour une question donn√©e"""
        
        try:
            # 1. G√©n√©rer l'embedding de la question
            question_embedding = self.embeddings.embed_query(question)
            
            # 2. Rechercher dans Pinecone
            search_results = self.index.query(
                vector=question_embedding,
                top_k=5,
                include_metadata=True
            )
            
            # 3. Extraire le contexte
            context_parts = []
            for match in search_results.matches:
                if 'text' in match.metadata:
                    context_parts.append(match.metadata['text'])
            
            context = "\n".join(context_parts)
            
            # 4. G√©n√©rer la r√©ponse avec le contexte
            prompt = f"""
            Tu es un expert en fact-checking sur le changement climatique.
            
            Contexte scientifique:
            {context}
            
            Question: {question}
            
            R√©ponds de mani√®re factuelle et pr√©cise en te basant sur le contexte fourni.
            Si le contexte ne contient pas d'information pertinente, indique-le clairement.
            
            R√©ponse:
            """
            
            response = await self.ollama_client.generate_response(prompt)
            return response
            
        except Exception as e:
            print(f"Erreur lors de la g√©n√©ration de r√©ponse RAG: {e}")
            return f"Erreur lors de la g√©n√©ration de r√©ponse: {e}"
    
    def create_llm_test_cases(self, test_cases):
        """Cr√©er des cas de test LLMTestCase pour DeepEval"""
        
        llm_test_cases = []
        
        for i, test_case in enumerate(test_cases):
            llm_test_cases.append(
                LLMTestCase(
                    input=test_case["question"],
                    actual_output="",  # Sera rempli lors de l'√©valuation
                    expected_output=test_case["expected_answer"],
                    context=test_case.get("category", "")
                )
            )
        
        return llm_test_cases
    
    async def evaluate_model(self, test_cases=None, max_questions=50):
        """√âvaluer le mod√®le avec DeepEval"""
        
        if test_cases is None:
            # Utiliser un sous-ensemble al√©atoire du dataset complet
            test_cases = get_random_subset(max_questions)
        
        llm_test_cases = self.create_llm_test_cases(test_cases)
        
        print(f"üß™ √âvaluation du fact-checker ({'RAG' if self.use_rag else 'Direct'})")
        print("=" * 60)
        print(f"üìã {len(test_cases)} cas de test")
        
        # G√©n√©rer les r√©ponses du mod√®le
        print("üîÑ G√©n√©ration des r√©ponses du mod√®le...")
        for i, test_case in enumerate(test_cases):
            print(f"  [{i+1}/{len(test_cases)}] {test_case['question'][:60]}...")
            response = await self.get_model_response(test_case['question'])
            llm_test_cases[i].actual_output = response
        
        # D√©finir les m√©triques d'√©valuation
        metrics = [
            AnswerRelevancy(threshold=0.7),
            ContextRelevancy(threshold=0.7),
            Faithfulness(threshold=0.7)
        ]
        
        # Lancer l'√©valuation
        print("\nüìä Lancement de l'√©valuation...")
        results = evaluate(
            llm_test_cases,
            metrics=metrics
        )
        
        # Afficher les r√©sultats
        print("\nüìà R√©sultats de l'√©valuation:")
        print("=" * 40)
        
        for metric in metrics:
            print(f"{metric.__class__.__name__}: {results[metric.__class__.__name__]:.3f}")
        
        # Analyse par cat√©gorie
        print("\nüìä Analyse par cat√©gorie:")
        categories = {}
        for test_case in test_cases:
            cat = test_case["category"]
            if cat not in categories:
                categories[cat] = []
            categories[cat].append(test_case)
        
        for cat, cases in categories.items():
            print(f"  {cat}: {len(cases)} questions")
        
        return results, test_cases, llm_test_cases

async def main():
    """Fonction principale"""
    
    print("üéØ √âvaluation compl√®te du fact-checker")
    print("=" * 50)
    
    # √âvaluation avec RAG
    print("\n1Ô∏è‚É£ √âvaluation avec RAG...")
    rag_evaluator = ComprehensiveFactCheckerEvaluator(use_rag=True)
    rag_results, rag_test_cases, rag_llm_cases = await rag_evaluator.evaluate_model(max_questions=30)
    
    # √âvaluation sans RAG
    print("\n2Ô∏è‚É£ √âvaluation sans RAG...")
    direct_evaluator = ComprehensiveFactCheckerEvaluator(use_rag=False)
    direct_results, direct_test_cases, direct_llm_cases = await direct_evaluator.evaluate_model(max_questions=30)
    
    # Comparaison des r√©sultats
    print("\nüìä Comparaison des r√©sultats:")
    print("=" * 40)
    
    metrics = ["AnswerRelevancy", "ContextRelevancy", "Faithfulness"]
    
    print(f"{'M√©trique':<20} {'RAG':<10} {'Direct':<10} {'Diff√©rence':<10}")
    print("-" * 50)
    
    for metric in metrics:
        rag_score = rag_results.get(metric, 0)
        direct_score = direct_results.get(metric, 0)
        diff = rag_score - direct_score
        
        print(f"{metric:<20} {rag_score:<10.3f} {direct_score:<10.3f} {diff:<+10.3f}")
    
    # Sauvegarder les r√©sultats d√©taill√©s
    detailed_results = {
        "rag_results": {
            "metrics": rag_results,
            "test_cases": [
                {
                    "question": tc["question"],
                    "expected": tc["expected_answer"],
                    "actual": llm_cases[i].actual_output,
                    "category": tc["category"]
                }
                for i, tc in enumerate(rag_test_cases)
            ]
        },
        "direct_results": {
            "metrics": direct_results,
            "test_cases": [
                {
                    "question": tc["question"],
                    "expected": tc["expected_answer"],
                    "actual": llm_cases[i].actual_output,
                    "category": tc["category"]
                }
                for i, tc in enumerate(direct_test_cases)
            ]
        }
    }
    
    with open("comprehensive_evaluation_results.json", "w", encoding="utf-8") as f:
        json.dump(detailed_results, f, indent=2, ensure_ascii=False)
    
    print("\nüíæ R√©sultats d√©taill√©s sauvegard√©s dans comprehensive_evaluation_results.json")
    print("\nüéâ √âvaluation termin√©e !")

if __name__ == "__main__":
    asyncio.run(main()) 