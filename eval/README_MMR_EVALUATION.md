# √âvaluation MMR (Maximum Marginal Relevance)

Ce dossier contient les scripts pour √©valuer votre mod√®le RAG avec MMR (Maximum Marginal Relevance) et comparer les performances avec la recherche simple.

## Scripts disponibles

### 1. `evaluate_rag_chromadb_mmr.py`

Script d'√©valuation complet avec MMR qui utilise DeepEval pour mesurer toutes les m√©triques importantes.

**Fonctionnalit√©s :**

- √âvaluation avec DeepEval (AnswerRelevancy, ContextualRelevancy, Faithfulness, etc.)
- Utilisation de MMR pour la s√©lection de documents
- Calcul des m√©triques de diversit√©
- Comparaison avec la recherche simple
- Sauvegarde des r√©sultats en JSON

### 2. `test_mmr_comparison.py`

Script de test rapide pour comparer MMR vs recherche simple sur quelques questions.

**Fonctionnalit√©s :**

- Test rapide avec 3-5 questions
- Comparaison des m√©triques de diversit√©
- Test avec diff√©rents param√®tres Œª
- Affichage d√©taill√© des r√©sultats

### 3. `run_mmr_evaluation.py`

Script principal pour ex√©cuter les √©valuations MMR avec diff√©rents param√®tres.

**Options :**

- `--lambda_param` : Param√®tre MMR (0.0 = max diversit√©, 1.0 = max pertinence)
- `--questions` : Nombre de questions √† tester
- `--comparison` : √âtude comparative avec diff√©rents Œª

## Utilisation

### Test rapide de comparaison

```bash
cd eval
python test_mmr_comparison.py
```

### √âvaluation compl√®te avec MMR

```bash
cd eval
python run_mmr_evaluation.py --lambda_param 0.5 --questions 10
```

### √âtude comparative compl√®te

```bash
cd eval
python run_mmr_evaluation.py --comparison
```

### √âvaluation directe

```bash
cd eval
python evaluate_rag_chromadb_mmr.py
```

## Param√®tres MMR

Le param√®tre Œª (lambda) contr√¥le l'√©quilibre entre pertinence et diversit√© :

- **Œª = 0.0** : Maximum de diversit√© (documents tr√®s diff√©rents)
- **Œª = 0.25** : Plus de diversit√© que de pertinence
- **Œª = 0.5** : √âquilibre entre pertinence et diversit√©
- **Œª = 0.75** : Plus de pertinence que de diversit√©
- **Œª = 1.0** : Maximum de pertinence (comme la recherche simple)

## M√©triques calcul√©es

### M√©triques DeepEval

- **AnswerRelevancy** : Pertinence de la r√©ponse
- **ContextualRelevancy** : Pertinence du contexte
- **Faithfulness** : Fid√©lit√© √† la source
- **ContextualRecall** : Rappel du contexte
- **ContextualPrecision** : Pr√©cision du contexte

### M√©triques MMR

- **Score de diversit√©** : Mesure de la diversit√© des documents s√©lectionn√©s
- **Similarit√© moyenne** : Similarit√© moyenne avec la requ√™te
- **Overlap ratio** : Proportion de documents communs avec la recherche simple
- **Score MMR** : Score combin√© pertinence + diversit√©

## Interpr√©tation des r√©sultats

### Comparaison MMR vs Recherche simple

1. **Diversit√©** : MMR devrait avoir un score de diversit√© plus √©lev√©
2. **Pertinence** : La recherche simple devrait avoir une similarit√© moyenne plus √©lev√©e
3. **Overlap** : Plus Œª est proche de 1.0, plus l'overlap avec la recherche simple est √©lev√©

### Choix du param√®tre Œª

- **Pour des questions complexes** : Œª = 0.25-0.5 (plus de diversit√©)
- **Pour des questions sp√©cifiques** : Œª = 0.75-1.0 (plus de pertinence)
- **√âquilibre g√©n√©ral** : Œª = 0.5

## Fichiers de r√©sultats

Les r√©sultats sont sauvegard√©s dans des fichiers JSON avec le format :

- `rag_eval_mmr_lambda{Œª}_{timestamp}.json`

Ces fichiers contiennent :

- R√©sultats DeepEval complets
- M√©triques MMR d√©taill√©es
- Analyse par cat√©gorie
- D√©tails de chaque question test√©e

## Exemple de sortie

```
üß™ √âvaluation du RAG avec ChromaDB et MMR
ü§ñ Mod√®le d'embedding: llama2:7b
üìä Param√®tre MMR (Œª): 0.5
üìã 10 cas de test
============================================================

üîç R√âSULTATS DEEPEVAL:
  AnswerRelevancy: 0.823
  ContextualRelevancy: 0.756
  Faithfulness: 0.891
  ContextualRecall: 0.734
  ContextualPrecision: 0.812

üìà M√âTRIQUES MMR:
  Similarit√© moyenne: 0.745
  Score MMR moyen: 0.678
  Diversit√© moyenne: 0.234
  Longueur contexte moyenne: 2847 caract√®res
  Overlap MMR/Simple: 0.456
```

## Troubleshooting

### Erreur "Aucun embedding trouv√©"

Assurez-vous que votre base de donn√©es ChromaDB contient des documents index√©s :

```bash
cd system
python index_documents.py
```

### Erreur de connexion Ollama

V√©rifiez qu'Ollama est en cours d'ex√©cution :

```bash
ollama serve
```

### Erreur de d√©pendances

Installez les d√©pendances d'√©valuation :

```bash
pip install -r requirements_eval.txt
```
