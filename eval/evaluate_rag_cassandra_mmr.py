#!/usr/bin/env python3
"""
Ã‰valuation complÃ¨te du RAG avec Cassandra et DeepEval
Teste toutes les mÃ©triques : RÃ©levance, FidÃ©litÃ©, RÃ©ponse, Contexte
"""

import os
import asyncio
import json
import time
from datetime import datetime
from typing import List, Dict, Any
from dotenv import load_dotenv

# DeepEval imports
from deepeval import evaluate
from deepeval.metrics import (
    AnswerRelevancyMetric, 
    ContextualRelevancyMetric, 
    FaithfulnessMetric,
    ContextualRecallMetric,
    ContextualPrecisionMetric
)
from deepeval.test_case import LLMTestCase
from deepeval.models import OllamaModel

# Local imports
import sys
sys.path.append('../system')
from cassandra_manager import create_cassandra_manager
from ollama_utils import OllamaClient, format_prompt, extract_verdict
from climate_dataset import CLIMATE_DATASET, get_dataset_by_category, get_random_subset

# Load environment variables
load_dotenv()

class ComprehensiveRAGEvaluator:
    """Ã‰valuateur complet pour le RAG avec Cassandra"""
    
    def __init__(self, embedding_model="llama2:7b"):
        """
        Initialiser l'Ã©valuateur
        
        Args:
            embedding_model: ModÃ¨le d'embedding Ã  utiliser
        """
        self.embedding_model = embedding_model
        
        # Initialiser les composants
        self.ollama_client = OllamaClient()
        self.cassandra_manager = create_cassandra_manager(
            table_name="fact_checker_docs"
        )
        
        # Prompts pour le fact-checking (version amÃ©liorÃ©e)
        self.fact_checking_prompt = """You are a decisive fact-checking expert on climate change.

Scientific context:
{context}

Question to verify:
{question}

CRITICAL VALIDATION: First, assess if the retrieved evidence actually addresses the specific claim made. If the evidence is NOT relevant to the specific claim, you MUST give an UNVERIFIABLE verdict.

IMPORTANT: You must be decisive and take a clear position. Do not hedge or be overly cautious. If the evidence supports the claim, say so. If it contradicts the claim, say so. If there's insufficient evidence, state that clearly.

IMPORTANT: Limit your answer to a maximum of 500 words.

Format your answer as follows:

ANALYSIS: [Your analysis based on the context]
VERDICT: [TRUE / MOSTLY TRUE / MOSTLY FALSE / FALSE / UNVERIFIABLE]
EXPLANATION: [Clear explanation of why you chose this verdict]
SOURCES: [References to the context used]
"""
    
    async def get_rag_response(self, question: str) -> Dict[str, Any]:
        """
        Obtenir une rÃ©ponse RAG complÃ¨te avec Cassandra
        
        Returns:
            Dict avec 'response', 'context', 'sources', 'similarity_scores'
        """
        try:
            # 1. Rechercher dans Cassandra avec MMR
            search_results = self.cassandra_manager.search_documents_mmr(
                question, 
                n_results=5, 
                lambda_param=0.7  # Plus de pertinence, moins de diversitÃ©
            )
            
            # 2. Extraire le contexte et les mÃ©tadonnÃ©es
            context_parts = []
            sources = []
            similarity_scores = []
            
            for doc in search_results:
                context_parts.append(doc['content'])
                sources.append({
                    'source': doc['metadata'].get('source', 'Unknown'),
                    'similarity': doc.get('similarity', 0.0)
                })
                similarity_scores.append(doc.get('similarity', 0.0))
            
            context = "\n\n".join(context_parts)
            
            # 3. GÃ©nÃ©rer la rÃ©ponse avec le contexte
            prompt = self.fact_checking_prompt.format(
                context=context,
                question=question
            )
            
            response = self.ollama_client.generate(prompt, temperature=0.2)
            
            return {
                'response': response,
                'context': context,
                'sources': sources,
                'similarity_scores': similarity_scores,
                'context_length': len(context)
            }
            
        except Exception as e:
            print(f"âŒ Erreur lors de la gÃ©nÃ©ration RAG: {e}")
            return {
                'response': f"Erreur: {e}",
                'context': "",
                'sources': [],
                'similarity_scores': [],
                'context_length': 0
            }
    
    def create_test_cases(self, dataset_subset: List[Dict]) -> List[LLMTestCase]:
        """CrÃ©er les cas de test pour DeepEval"""
        test_cases = []
        
        for test_case in dataset_subset:
            test_cases.append(
                LLMTestCase(
                    input=test_case["question"],
                    actual_output="",  # Sera rempli lors de l'Ã©valuation
                    expected_output=test_case["expected_answer"],
                    context=[test_case.get("category", "")],  # Doit Ãªtre une liste
                    retrieval_context=[]  # Sera rempli lors de l'Ã©valuation
                )
            )
        
        return test_cases
    
    async def evaluate_rag_system(self, test_cases: List[Dict] = None, max_questions: int = 50) -> Dict[str, Any]:
        """
        Ã‰valuer le systÃ¨me RAG complet
        
        Args:
            test_cases: Cas de test personnalisÃ©s
            max_questions: Nombre maximum de questions Ã  Ã©valuer
            
        Returns:
            RÃ©sultats complets de l'Ã©valuation
        """
        if test_cases is None:
            test_cases = get_random_subset(max_questions)
        
        print(f"ðŸ§ª Ã‰valuation du RAG avec Cassandra")
        print(f"ðŸ¤– ModÃ¨le d'embedding: {self.embedding_model}")
        print(f"ðŸ“Š Base de donnÃ©es: Cassandra (5,233 documents)")
        print(f"ðŸ“‹ {len(test_cases)} cas de test")
        print("=" * 60)
        
        # CrÃ©er les cas de test DeepEval
        llm_test_cases = self.create_test_cases(test_cases)
        
        # GÃ©nÃ©rer les rÃ©ponses du modÃ¨le
        print("ðŸ”„ GÃ©nÃ©ration des rÃ©ponses RAG...")
        rag_details = []
        
        for i, test_case in enumerate(test_cases):
            print(f"  [{i+1}/{len(test_cases)}] {test_case['question'][:60]}...")
            
            # Obtenir la rÃ©ponse RAG
            rag_result = await self.get_rag_response(test_case['question'])
            
            # Mettre Ã  jour le cas de test
            llm_test_cases[i].actual_output = rag_result['response']
            llm_test_cases[i].retrieval_context = [rag_result['context']]  # Doit Ãªtre une liste
            
            # Stocker les dÃ©tails pour l'analyse
            rag_details.append({
                'question': test_case['question'],
                'expected': test_case['expected_answer'],
                'actual': rag_result['response'],
                'category': test_case['category'],
                'context': rag_result['context'],
                'sources': rag_result['sources'],
                'similarity_scores': rag_result['similarity_scores'],
                'context_length': rag_result['context_length']
            })
        
        # Configurer le modÃ¨le Ollama pour DeepEval
        ollama_model = OllamaModel(
            model="llama2:7b",
            base_url="http://localhost:11434",
            temperature=0.1  # LÃ©gÃ¨rement plus dÃ©cisif
        )
        
        # DÃ©finir toutes les mÃ©triques d'Ã©valuation avec Ollama
        metrics = [
            AnswerRelevancyMetric(threshold=0.7, model=ollama_model, include_reason=True),
            ContextualRelevancyMetric(threshold=0.7, model=ollama_model),
            FaithfulnessMetric(threshold=0.7, model=ollama_model),
            ContextualRecallMetric(threshold=0.7, model=ollama_model),
            ContextualPrecisionMetric(threshold=0.7, model=ollama_model)
        ]
        
        # Lancer l'Ã©valuation
        print("\nðŸ“Š Lancement de l'Ã©valuation DeepEval...")
        start_time = time.time()
        
        results = evaluate(
            llm_test_cases,
            metrics=metrics
        )
        
        evaluation_time = time.time() - start_time
        
        # Analyser les rÃ©sultats par catÃ©gorie
        category_analysis = self.analyze_by_category(rag_details)
        
        # Calculer des mÃ©triques supplÃ©mentaires
        additional_metrics = self.calculate_additional_metrics(rag_details)
        
        # Compiler les rÃ©sultats complets
        comprehensive_results = {
            'evaluation_time': evaluation_time,
            'deep_eval_metrics': results,
            'additional_metrics': additional_metrics,
            'category_analysis': category_analysis,
            'test_details': rag_details,
            'model_info': {
                'embedding_model': self.embedding_model,
                'total_questions': len(test_cases),
                'categories_tested': list(set(tc['category'] for tc in test_cases))
            }
        }
        
        return comprehensive_results
    
    def analyze_by_category(self, rag_details: List[Dict]) -> Dict[str, Dict]:
        """Analyser les rÃ©sultats par catÃ©gorie"""
        categories = {}
        
        for detail in rag_details:
            cat = detail['category']
            if cat not in categories:
                categories[cat] = {
                    'count': 0,
                    'avg_similarity': 0,
                    'avg_context_length': 0,
                    'questions': []
                }
            
            categories[cat]['count'] += 1
            categories[cat]['avg_similarity'] += sum(detail['similarity_scores']) / len(detail['similarity_scores']) if detail['similarity_scores'] else 0
            categories[cat]['avg_context_length'] += detail['context_length']
            categories[cat]['questions'].append(detail['question'])
        
        # Calculer les moyennes
        for cat in categories:
            count = categories[cat]['count']
            categories[cat]['avg_similarity'] /= count
            categories[cat]['avg_context_length'] /= count
        
        return categories
    
    def calculate_additional_metrics(self, rag_details: List[Dict]) -> Dict[str, float]:
        """Calculer des mÃ©triques supplÃ©mentaires"""
        total_questions = len(rag_details)
        
        # MÃ©triques de contexte
        avg_context_length = sum(d['context_length'] for d in rag_details) / total_questions
        avg_similarity = sum(
            sum(d['similarity_scores']) / len(d['similarity_scores']) 
            for d in rag_details if d['similarity_scores']
        ) / total_questions
        
        # MÃ©triques de sources
        unique_sources = set()
        for detail in rag_details:
            for source in detail['sources']:
                unique_sources.add(source['source'])
        
        return {
            'avg_context_length': avg_context_length,
            'avg_similarity_score': avg_similarity,
            'unique_sources_used': len(unique_sources),
            'total_questions': total_questions
        }
    
    def print_results(self, results: Dict[str, Any]):
        """Afficher les rÃ©sultats de maniÃ¨re formatÃ©e"""
        print("\n" + "="*60)
        print("ðŸ“ˆ RÃ‰SULTATS DE L'Ã‰VALUATION RAG")
        print("="*60)
        
        # MÃ©triques DeepEval
        print("\nðŸŽ¯ MÃ‰TRIQUES DEEPEVAL:")
        print("-" * 30)
        # AccÃ©der aux mÃ©triques via l'objet EvaluationResult
        deep_eval_results = results['deep_eval_metrics']
        if hasattr(deep_eval_results, 'metric_scores'):
            for metric_name, score in deep_eval_results.metric_scores.items():
                print(f"{metric_name:<20}: {score:.3f}")
        else:
            print("RÃ©sultats DeepEval disponibles mais format non standard")
            print(f"Type: {type(deep_eval_results)}")
            print(f"Contenu: {deep_eval_results}")
        
        # MÃ©triques supplÃ©mentaires
        print("\nðŸ“Š MÃ‰TRIQUES SUPPLÃ‰MENTAIRES:")
        print("-" * 30)
        additional = results['additional_metrics']
        print(f"Longueur moyenne contexte: {additional['avg_context_length']:.0f} caractÃ¨res")
        print(f"Score de similaritÃ© moyen: {additional['avg_similarity_score']:.3f}")
        print(f"Sources uniques utilisÃ©es: {additional['unique_sources_used']}")
        print(f"Temps d'Ã©valuation: {results['evaluation_time']:.1f} secondes")
        
        # Analyse par catÃ©gorie
        print("\nðŸ“‹ ANALYSE PAR CATÃ‰GORIE:")
        print("-" * 30)
        for category, stats in results['category_analysis'].items():
            print(f"{category:<20}: {stats['count']} questions, "
                  f"similaritÃ© {stats['avg_similarity']:.3f}, "
                  f"contexte {stats['avg_context_length']:.0f} chars")
    
    def save_results(self, results: Dict[str, Any], filename: str = None):
        """Sauvegarder les rÃ©sultats dans un fichier JSON"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"rag_evaluation_results_{timestamp}.json"
        
        # Convertir l'objet EvaluationResult en dictionnaire
        deep_eval_results = results['deep_eval_metrics']
        if hasattr(deep_eval_results, 'metric_scores'):
            deep_eval_dict = deep_eval_results.metric_scores
        else:
            deep_eval_dict = str(deep_eval_results)  # Fallback
        
        # Nettoyer les rÃ©sultats pour la sÃ©rialisation JSON
        clean_results = {
            'evaluation_time': results['evaluation_time'],
            'deep_eval_metrics': deep_eval_dict,
            'additional_metrics': results['additional_metrics'],
            'category_analysis': results['category_analysis'],
            'model_info': results['model_info']
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(clean_results, f, indent=2, ensure_ascii=False)
        
        print(f"\nðŸ’¾ RÃ©sultats sauvegardÃ©s dans: {filename}")
        return filename

async def main():
    """Fonction principale d'Ã©valuation"""
    
    print("ðŸŽ¯ Ã‰VALUATION COMPLÃˆTE DU RAG AVEC CASSANDRA")
    print("=" * 60)
    
    # Initialiser l'Ã©valuateur
    evaluator = ComprehensiveRAGEvaluator(embedding_model="llama2:7b")
    
    # VÃ©rifier que Cassandra contient des documents
    collection_info = evaluator.cassandra_manager.get_collection_info()
    if not collection_info.get('index_loaded', False):
        print("âŒ Aucun document dans Cassandra. Veuillez d'abord indexer vos documents.")
        return
    
    print(f"âœ… Cassandra contient des documents (base vectorielle prÃªte)")
    
    # Ã‰valuer avec diffÃ©rentes tailles de dataset
    test_sizes = [10]  # Commencer petit pour tester
    
    for size in test_sizes:
        print(f"\nðŸ§ª Test avec {size} questions...")
        
        # Obtenir un sous-ensemble du dataset
        test_cases = get_random_subset(size)
        
        # Ã‰valuer
        results = await evaluator.evaluate_rag_system(test_cases)
        
        # Afficher les rÃ©sultats
        evaluator.print_results(results)
        
        # Sauvegarder
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"rag_eval_cassandra_{size}q_{timestamp}.json"
        evaluator.save_results(results, filename)
        
        print(f"\nâœ… Ã‰valuation avec {size} questions terminÃ©e!")
        
        # Pause entre les tests
        if size != test_sizes[-1]:
            print("â³ Pause de 5 secondes avant le prochain test...")
            await asyncio.sleep(5)
    
    print("\nðŸŽ‰ Ã‰valuation complÃ¨te terminÃ©e !")

if __name__ == "__main__":
    asyncio.run(main()) 