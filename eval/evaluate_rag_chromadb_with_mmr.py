#!/usr/bin/env python3
"""
√âvaluation compl√®te du RAG avec ChromaDB et MMR
Teste toutes les m√©triques : R√©levance, Fid√©lit√©, R√©ponse, Contexte
"""

import os
import asyncio
import json
import time
from datetime import datetime
from typing import List, Dict, Any
from dotenv import load_dotenv

# DeepEval imports
from deepeval import evaluate
from deepeval.metrics import (
    AnswerRelevancyMetric, 
    ContextualRelevancyMetric, 
    FaithfulnessMetric,
    ContextualRecallMetric,
    ContextualPrecisionMetric
)
from deepeval.test_case import LLMTestCase
from deepeval.models import OllamaModel

# Local imports
import sys
sys.path.append('../system')
from chromadb_manager import ChromaDBManager
from ollama_utils import OllamaClient, format_prompt, extract_verdict
from climate_dataset import CLIMATE_DATASET, get_dataset_by_category, get_random_subset
from mmr_utils import mmr_similarity_search, simple_similarity_search, calculate_diversity_metrics

# Load environment variables
load_dotenv()

class ChromaDBMMREvaluator:
    """√âvaluateur complet pour le RAG avec ChromaDB et MMR"""
    
    def __init__(self, embedding_model="llama2:7b", lambda_param=0.5):
        """
        Initialiser l'√©valuateur
        
        Args:
            embedding_model: Mod√®le d'embedding √† utiliser
            lambda_param: Param√®tre MMR (0.0 = max diversit√©, 1.0 = max pertinence)
        """
        self.embedding_model = embedding_model
        self.lambda_param = lambda_param
        
        # Initialiser les composants
        self.ollama_client = OllamaClient()
        self.chroma_manager = ChromaDBManager(
            persist_directory="../system/chroma_db",
            embedding_model=embedding_model
        )
        
        # Prompts pour le fact-checking
        self.fact_checking_prompt = """Tu es un expert en fact-checking sur le changement climatique.

Contexte scientifique:
{context}

Question √† v√©rifier: {question}

Analyse cette question en te basant UNIQUEMENT sur le contexte fourni.
Si le contexte ne contient pas d'information pertinente, indique-le clairement.

IMPORTANT: Limite ta r√©ponse √† 500 mots maximum.

Format ta r√©ponse comme suit:
ANALYSE: [Ton analyse bas√©e sur le contexte]
VERDICT: [VRAI/FAUX/MIXTE/INCONNU]
EXPLICATION: [Explication de ton verdict]
SOURCES: [R√©f√©rences au contexte utilis√©]
"""
    
    def _cosine_similarity(self, a: List[float], b: List[float]) -> float:
        """Calculer la similarit√© cosinus entre deux vecteurs"""
        import numpy as np
        # Convert to numpy arrays if they aren't already
        if not isinstance(a, np.ndarray):
            a = np.array(a)
        if not isinstance(b, np.ndarray):
            b = np.array(b)
        
        # Handle zero vectors
        norm_a = np.linalg.norm(a)
        norm_b = np.linalg.norm(b)
        
        if norm_a == 0 or norm_b == 0:
            return 0.0
        
        return float(np.dot(a, b) / (norm_a * norm_b))
    
    async def get_rag_response_with_mmr(self, question: str, n_results: int = 5) -> Dict[str, Any]:
        """
        Obtenir une r√©ponse RAG compl√®te avec MMR sur ChromaDB
        
        Returns:
            Dict avec 'response', 'context', 'sources', 'similarity_scores', 'mmr_scores'
        """
        try:
            # 1. Obtenir tous les embeddings de la collection
            results = self.chroma_manager.collection.get(
                include=['embeddings', 'documents', 'metadatas']
            )
            
            if not results['embeddings'] or len(results['embeddings']) == 0:
                print("‚ùå Aucun embedding trouv√© dans la collection")
                return self._error_response("Aucun embedding disponible")
            
            # Convertir les embeddings en listes si ce sont des numpy arrays
            import numpy as np
            all_embeddings = []
            for emb in results['embeddings']:
                if isinstance(emb, np.ndarray):
                    all_embeddings.append(emb.tolist())
                else:
                    all_embeddings.append(emb)
            
            all_documents = results['documents']
            all_metadatas = results['metadatas']
            
            # 2. G√©n√©rer l'embedding de la requ√™te
            query_embedding = self.chroma_manager.embeddings.embed_query(question)
            
            # 3. Appliquer MMR pour s√©lectionner les documents
            mmr_indices = mmr_similarity_search(
                all_embeddings, 
                query_embedding, 
                k=n_results, 
                lambda_param=self.lambda_param
            )
            
            # 4. Comparer avec la recherche simple pour l'analyse
            simple_indices = simple_similarity_search(
                all_embeddings, 
                query_embedding, 
                k=n_results
            )
            
            # 5. Extraire les documents s√©lectionn√©s par MMR
            context_parts = []
            sources = []
            similarity_scores = []
            mmr_scores = []
            
            for idx in mmr_indices:
                content = all_documents[idx]
                metadata = all_metadatas[idx]
                
                # Calculer la similarit√© avec la requ√™te
                doc_embedding = all_embeddings[idx]
                similarity = self._cosine_similarity(query_embedding, doc_embedding)
                
                context_parts.append(content)
                sources.append({
                    'source': metadata.get('source', 'Unknown'),
                    'similarity': similarity,
                    'mmr_index': idx
                })
                similarity_scores.append(similarity)
                
                # Calculer le score MMR pour ce document
                selected_embeddings = [all_embeddings[i] for i in mmr_indices[:mmr_indices.index(idx)]]
                mmr_score = self._calculate_mmr_score(query_embedding, doc_embedding, selected_embeddings)
                mmr_scores.append(mmr_score)
            
            context = "\n\n".join(context_parts)
            
            # 6. Calculer les m√©triques de diversit√©
            selected_embeddings = [all_embeddings[i] for i in mmr_indices]
            diversity_metrics = calculate_diversity_metrics(selected_embeddings)
            
            # 7. G√©n√©rer la r√©ponse avec le contexte
            prompt = self.fact_checking_prompt.format(
                context=context,
                question=question
            )
            
            response = self.ollama_client.generate(prompt, temperature=0.2)
            
            return {
                'response': response,
                'context': context,
                'sources': sources,
                'similarity_scores': similarity_scores,
                'mmr_scores': mmr_scores,
                'context_length': len(context),
                'mmr_indices': mmr_indices,
                'simple_indices': simple_indices,
                'diversity_metrics': diversity_metrics,
                'lambda_param': self.lambda_param
            }
            
        except Exception as e:
            print(f"‚ùå Erreur lors de la g√©n√©ration RAG avec MMR: {e}")
            return self._error_response(str(e))
    
    def _calculate_mmr_score(self, query_embedding: List[float], doc_embedding: List[float], 
                           selected_embeddings: List[List[float]]) -> float:
        """Calculer le score MMR pour un document"""
        # Relevance score (similarity to query)
        relevance = self._cosine_similarity(query_embedding, doc_embedding)
        
        # Diversity score (minimum similarity to already selected documents)
        if not selected_embeddings:
            diversity = 1.0  # First document has maximum diversity
        else:
            similarities = [self._cosine_similarity(doc_embedding, selected_emb) for selected_emb in selected_embeddings]
            diversity = 1.0 - max(similarities)  # 1 - max similarity = diversity
        
        # MMR score = Œª * relevance + (1-Œª) * diversity
        mmr_score = self.lambda_param * relevance + (1 - self.lambda_param) * diversity
        
        return mmr_score
    
    def _error_response(self, error_msg: str) -> Dict[str, Any]:
        """Cr√©er une r√©ponse d'erreur standardis√©e"""
        return {
            'response': f"Erreur: {error_msg}",
            'context': "",
            'sources': [],
            'similarity_scores': [],
            'mmr_scores': [],
            'context_length': 0,
            'mmr_indices': [],
            'simple_indices': [],
            'diversity_metrics': {},
            'lambda_param': self.lambda_param
        }
    
    def create_test_cases(self, dataset_subset: List[Dict]) -> List[LLMTestCase]:
        """Cr√©er les cas de test pour DeepEval"""
        test_cases = []
        
        for test_case in dataset_subset:
            test_cases.append(
                LLMTestCase(
                    input=test_case["question"],
                    actual_output="",  # Sera rempli lors de l'√©valuation
                    expected_output=test_case["expected_answer"],
                    context=[test_case.get("category", "")],  # Doit √™tre une liste
                    retrieval_context=[]  # Sera rempli lors de l'√©valuation
                )
            )
        
        return test_cases
    
    async def evaluate_rag_system(self, test_cases: List[Dict] = None, max_questions: int = 10) -> Dict[str, Any]:
        """
        √âvaluer le syst√®me RAG complet avec MMR
        
        Args:
            test_cases: Cas de test personnalis√©s
            max_questions: Nombre maximum de questions √† √©valuer
            
        Returns:
            R√©sultats complets de l'√©valuation
        """
        if test_cases is None:
            test_cases = get_random_subset(max_questions)
        
        print(f"üß™ √âvaluation du RAG avec ChromaDB et MMR")
        print(f"ü§ñ Mod√®le d'embedding: {self.embedding_model}")
        print(f"üìä Param√®tre MMR (Œª): {self.lambda_param}")
        print(f"üìã {len(test_cases)} cas de test")
        print("=" * 60)
        
        # Cr√©er les cas de test DeepEval
        llm_test_cases = self.create_test_cases(test_cases)
        
        # G√©n√©rer les r√©ponses du mod√®le
        print("üîÑ G√©n√©ration des r√©ponses RAG avec MMR...")
        rag_details = []
        
        for i, test_case in enumerate(test_cases):
            print(f"  [{i+1}/{len(test_cases)}] {test_case['question'][:60]}...")
            
            # Obtenir la r√©ponse RAG avec MMR
            rag_result = await self.get_rag_response_with_mmr(test_case['question'])
            
            # Mettre √† jour le cas de test
            llm_test_cases[i].actual_output = rag_result['response']
            llm_test_cases[i].retrieval_context = [rag_result['context']]  # Doit √™tre une liste
            
            # Stocker les d√©tails pour l'analyse
            rag_details.append({
                'question': test_case['question'],
                'expected': test_case['expected_answer'],
                'actual': rag_result['response'],
                'category': test_case['category'],
                'context': rag_result['context'],
                'sources': rag_result['sources'],
                'similarity_scores': rag_result['similarity_scores'],
                'mmr_scores': rag_result['mmr_scores'],
                'context_length': rag_result['context_length'],
                'mmr_indices': rag_result['mmr_indices'],
                'simple_indices': rag_result['simple_indices'],
                'diversity_metrics': rag_result['diversity_metrics'],
                'lambda_param': rag_result['lambda_param']
            })
        
        # Configurer le mod√®le Ollama pour DeepEval
        ollama_model = OllamaModel(
            model="llama2:7b",
            base_url="http://localhost:11434",
            temperature=0
        )
        
        # D√©finir toutes les m√©triques d'√©valuation avec Ollama
        metrics = [
            AnswerRelevancyMetric(model=ollama_model),
            ContextualRelevancyMetric(model=ollama_model),
            FaithfulnessMetric(model=ollama_model),
            ContextualRecallMetric(model=ollama_model),
            ContextualPrecisionMetric(model=ollama_model)
        ]
        
        # √âvaluer avec DeepEval
        print("üìä √âvaluation avec DeepEval...")
        results = await evaluate(
            test_cases=llm_test_cases,
            metrics=metrics
        )
        
        # Analyser les r√©sultats par cat√©gorie
        category_analysis = self.analyze_by_category(rag_details)
        
        # Calculer des m√©triques suppl√©mentaires
        additional_metrics = self.calculate_additional_metrics(rag_details)
        
        # Compiler tous les r√©sultats
        final_results = {
            'evaluation_date': datetime.now().isoformat(),
            'model': self.embedding_model,
            'lambda_param': self.lambda_param,
            'deepeval_results': results,
            'category_analysis': category_analysis,
            'additional_metrics': additional_metrics,
            'rag_details': rag_details,
            'test_cases_count': len(test_cases)
        }
        
        return final_results
    
    def analyze_by_category(self, rag_details: List[Dict]) -> Dict[str, Dict]:
        """Analyser les r√©sultats par cat√©gorie"""
        categories = {}
        
        for detail in rag_details:
            category = detail['category']
            if category not in categories:
                categories[category] = {
                    'count': 0,
                    'avg_similarity': 0,
                    'avg_mmr_score': 0,
                    'avg_diversity_score': 0,
                    'avg_context_length': 0
                }
            
            categories[category]['count'] += 1
            categories[category]['avg_similarity'] += np.mean(detail['similarity_scores'])
            categories[category]['avg_mmr_score'] += np.mean(detail['mmr_scores'])
            categories[category]['avg_diversity_score'] += detail['diversity_metrics'].get('diversity_score', 0)
            categories[category]['avg_context_length'] += detail['context_length']
        
        # Calculer les moyennes
        for category in categories:
            count = categories[category]['count']
            categories[category]['avg_similarity'] /= count
            categories[category]['avg_mmr_score'] /= count
            categories[category]['avg_diversity_score'] /= count
            categories[category]['avg_context_length'] /= count
        
        return categories
    
    def calculate_additional_metrics(self, rag_details: List[Dict]) -> Dict[str, float]:
        """Calculer des m√©triques suppl√©mentaires"""
        import numpy as np
        
        # M√©triques globales
        avg_similarity = np.mean([np.mean(detail['similarity_scores']) for detail in rag_details])
        avg_mmr_score = np.mean([np.mean(detail['mmr_scores']) for detail in rag_details])
        avg_diversity = np.mean([detail['diversity_metrics'].get('diversity_score', 0) for detail in rag_details])
        avg_context_length = np.mean([detail['context_length'] for detail in rag_details])
        
        # Calculer l'overlap entre MMR et recherche simple
        mmr_simple_overlap = 0
        for detail in rag_details:
            mmr_set = set(detail['mmr_indices'])
            simple_set = set(detail['simple_indices'])
            overlap = len(mmr_set.intersection(simple_set))
            mmr_simple_overlap += overlap / len(mmr_set) if mmr_set else 0
        
        avg_overlap = mmr_simple_overlap / len(rag_details)
        
        return {
            'avg_similarity_score': avg_similarity,
            'avg_mmr_score': avg_mmr_score,
            'avg_diversity_score': avg_diversity,
            'avg_context_length': avg_context_length,
            'mmr_simple_overlap_ratio': avg_overlap
        }
    
    def print_results(self, results: Dict[str, Any]):
        """Afficher les r√©sultats de l'√©valuation"""
        print("\n" + "="*60)
        print("üìä R√âSULTATS DE L'√âVALUATION MMR AVEC CHROMADB")
        print("="*60)
        
        # Informations g√©n√©rales
        print(f"üìÖ Date: {results['evaluation_date']}")
        print(f"ü§ñ Mod√®le: {results['model']}")
        print(f"üìä Param√®tre MMR (Œª): {results['lambda_param']}")
        print(f"üìã Nombre de tests: {results['test_cases_count']}")
        
        # R√©sultats DeepEval
        print("\nüîç R√âSULTATS DEEPEVAL:")
        deepeval_results = results['deepeval_results']
        for metric_name, score in deepeval_results.items():
            print(f"  {metric_name}: {score:.3f}")
        
        # M√©triques suppl√©mentaires
        print("\nüìà M√âTRIQUES MMR:")
        additional_metrics = results['additional_metrics']
        print(f"  Similarit√© moyenne: {additional_metrics['avg_similarity_score']:.3f}")
        print(f"  Score MMR moyen: {additional_metrics['avg_mmr_score']:.3f}")
        print(f"  Diversit√© moyenne: {additional_metrics['avg_diversity_score']:.3f}")
        print(f"  Longueur contexte moyenne: {additional_metrics['avg_context_length']:.0f} caract√®res")
        print(f"  Overlap MMR/Simple: {additional_metrics['mmr_simple_overlap_ratio']:.3f}")
        
        # Analyse par cat√©gorie
        print("\nüìÇ ANALYSE PAR CAT√âGORIE:")
        category_analysis = results['category_analysis']
        for category, metrics in category_analysis.items():
            print(f"  {category}:")
            print(f"    Nombre de tests: {metrics['count']}")
            print(f"    Similarit√© moyenne: {metrics['avg_similarity']:.3f}")
            print(f"    Score MMR moyen: {metrics['avg_mmr_score']:.3f}")
            print(f"    Diversit√© moyenne: {metrics['avg_diversity_score']:.3f}")
    
    def save_results(self, results: Dict[str, Any], filename: str = None):
        """Sauvegarder les r√©sultats dans un fichier JSON"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"rag_eval_chromadb_mmr_lambda{self.lambda_param}_{timestamp}.json"
        
        filepath = os.path.join("eval", filename)
        
        # Convertir les numpy arrays en listes pour la s√©rialisation JSON
        def convert_numpy(obj):
            import numpy as np
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            return obj
        
        # Convertir r√©cursivement
        def convert_recursive(obj):
            if isinstance(obj, dict):
                return {k: convert_recursive(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_recursive(v) for v in obj]
            else:
                return convert_numpy(obj)
        
        results_converted = convert_recursive(results)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results_converted, f, indent=2, ensure_ascii=False)
        
        print(f"üíæ R√©sultats sauvegard√©s dans: {filepath}")
        return filepath

async def main():
    """Fonction principale pour l'√©valuation MMR avec ChromaDB"""
    print("üöÄ D√©marrage de l'√©valuation RAG avec MMR et ChromaDB")
    
    # Param√®tres d'√©valuation
    embedding_model = "llama2:7b"
    lambda_param = 0.5  # Param√®tre MMR (0.0 = max diversit√©, 1.0 = max pertinence)
    max_questions = 10  # Nombre de questions √† tester
    
    # Cr√©er l'√©valuateur
    evaluator = ChromaDBMMREvaluator(
        embedding_model=embedding_model,
        lambda_param=lambda_param
    )
    
    # √âvaluer le syst√®me
    results = await evaluator.evaluate_rag_system(max_questions=max_questions)
    
    # Afficher les r√©sultats
    evaluator.print_results(results)
    
    # Sauvegarder les r√©sultats
    evaluator.save_results(results)
    
    print("\n‚úÖ √âvaluation MMR avec ChromaDB termin√©e!")

if __name__ == "__main__":
    import numpy as np
    asyncio.run(main()) 