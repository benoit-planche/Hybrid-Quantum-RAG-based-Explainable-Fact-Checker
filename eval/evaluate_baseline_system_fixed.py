#!/usr/bin/env python3
"""
√âvaluation compl√®te du syst√®me de base (classique) - Version corrig√©e
"""

import sys
import os
sys.path.append('system')
sys.path.append('src/quantum')

import time
import numpy as np
from datetime import datetime
from system.cassandra_manager import CassandraVectorStoreManager

def evaluate_baseline_system_fixed():
    """√âvaluation compl√®te du syst√®me de base classique avec gestion d'erreurs"""
    
    print("üöÄ √âvaluation du syst√®me de base (classique) - Version corrig√©e")
    print("=" * 70)
    
    # Connexion √† Cassandra
    cassandra_manager = CassandraVectorStoreManager()
    
    # Requ√™tes de test identiques √† l'√©valuation quantique
    test_queries = [
        "climate change",
        "global warming", 
        "carbon emissions",
        "renewable energy",
        "greenhouse gases",
        "ocean acidification",
        "melting ice caps",
        "solar power",
        "wind energy",
        "biodiversity loss"
    ]
    
    print(f"üìã {len(test_queries)} requ√™tes de test")
    print(f"üîç Syst√®me: Classique (Cassandra + Ollama)")
    print(f"ü§ñ Mod√®le: {cassandra_manager.embedding_model}")
    print()
    
    # M√©triques de collecte
    all_times = []
    all_similarities = []
    all_results = []
    
    # Test de chaque requ√™te
    for i, query in enumerate(test_queries, 1):
        print(f"üîç Requ√™te {i}/{len(test_queries)}: '{query}'")
        
        start_time = time.time()
        
        try:
            # Essayer d'abord la recherche MMR
            try:
                results = cassandra_manager.search_documents_mmr(
                    query=query,
                    n_results=10,
                    lambda_param=0.5
                )
            except Exception as e:
                print(f"   ‚ö†Ô∏è MMR √©chou√©, tentative recherche simple: {e}")
                # Fallback vers recherche simple
                results = cassandra_manager.search_documents_simple(
                    query=query,
                    n_results=10
                )
            
            end_time = time.time()
            query_time = end_time - start_time
            
            # Extraction des similarit√©s avec gestion d'erreurs
            similarities = []
            valid_results = []
            
            for result in results:
                try:
                    # Essayer diff√©rentes cl√©s pour les scores
                    score = None
                    if 'score' in result:
                        score = result['score']
                    elif 'similarity' in result:
                        score = result['similarity']
                    elif 'distance' in result:
                        # Convertir la distance en similarit√©
                        score = 1.0 - result['distance']
                    else:
                        # Score par d√©faut
                        score = 0.5
                    
                    # Validation du score
                    if score is not None and 0 <= score <= 1:
                        similarities.append(score)
                        valid_results.append(result)
                    
                except Exception as e:
                    print(f"   ‚ö†Ô∏è Erreur traitement r√©sultat: {e}")
                    continue
            
            # Statistiques de la requ√™te
            if similarities:
                max_sim = max(similarities)
                min_sim = min(similarities)
                mean_sim = np.mean(similarities)
            else:
                max_sim = min_sim = mean_sim = 0
            
            print(f"   ‚è±Ô∏è Temps: {query_time:.2f}s")
            print(f"   üìä R√©sultats: {len(valid_results)}")
            print(f"   üéØ Score max: {max_sim:.4f}")
            print(f"   üéØ Score min: {min_sim:.4f}")
            print(f"   üéØ Score moyen: {mean_sim:.4f}")
            
            # Affichage des top 3 r√©sultats
            print(f"   üìã Top 3 r√©sultats:")
            for j, result in enumerate(valid_results[:3], 1):
                doc_id = result.get('id', result.get('doc_id', f'Doc_{j}'))
                score = similarities[j-1] if j <= len(similarities) else 0
                print(f"      {j}. {doc_id}: {score:.4f}")
            
            # Stockage des m√©triques
            all_times.append(query_time)
            all_similarities.extend(similarities)
            all_results.append({
                'query': query,
                'time': query_time,
                'results_count': len(valid_results),
                'similarities': similarities,
                'max_sim': max_sim,
                'min_sim': min_sim,
                'mean_sim': mean_sim
            })
            
        except Exception as e:
            print(f"   ‚ùå Erreur compl√®te: {e}")
            all_times.append(0)
            all_results.append({
                'query': query,
                'time': 0,
                'results_count': 0,
                'similarities': [],
                'max_sim': 0,
                'min_sim': 0,
                'mean_sim': 0
            })
        
        print()
    
    # Calcul des m√©triques globales
    total_time = sum(all_times)
    avg_time = np.mean(all_times)
    total_results = sum(r['results_count'] for r in all_results)
    avg_results = total_results / len(test_queries)
    
    # Statistiques des similarit√©s
    if all_similarities:
        mean_similarity = np.mean(all_similarities)
        std_similarity = np.std(all_similarities)
        min_similarity = min(all_similarities)
        max_similarity = max(all_similarities)
    else:
        mean_similarity = std_similarity = min_similarity = max_similarity = 0
    
    # Distribution des similarit√©s
    similarity_ranges = {
        '0.0-0.2': 0,
        '0.2-0.4': 0,
        '0.4-0.6': 0,
        '0.6-0.8': 0,
        '0.8-1.0': 0
    }
    
    for sim in all_similarities:
        if sim < 0.2:
            similarity_ranges['0.0-0.2'] += 1
        elif sim < 0.4:
            similarity_ranges['0.2-0.4'] += 1
        elif sim < 0.6:
            similarity_ranges['0.4-0.6'] += 1
        elif sim < 0.8:
            similarity_ranges['0.6-0.8'] += 1
        else:
            similarity_ranges['0.8-1.0'] += 1
    
    # Calcul de la diff√©renciation
    unique_scores = len(set(all_similarities))
    differentiation_ratio = unique_scores / len(all_similarities) if all_similarities else 0
    
    # Affichage des r√©sultats globaux
    print("üìä M√âTRIQUES GLOBALES")
    print("=" * 60)
    print(f"‚è±Ô∏è Temps total: {total_time:.2f}s")
    print(f"‚è±Ô∏è Temps moyen par requ√™te: {avg_time:.2f}s")
    print(f"üìä Nombre total de r√©sultats: {total_results}")
    print(f"üìä R√©sultats moyens par requ√™te: {avg_results:.1f}")
    print(f"üéØ Similarit√© moyenne: {mean_similarity:.4f}")
    print(f"üéØ √âcart-type similarit√©: {std_similarity:.4f}")
    print(f"üéØ Similarit√© min: {min_similarity:.4f}")
    print(f"üéØ Similarit√© max: {max_similarity:.4f}")
    print()
    
    print("üìà DISTRIBUTION DES SIMILARIT√âS")
    print("-" * 40)
    for range_name, count in similarity_ranges.items():
        percentage = (count / len(all_similarities) * 100) if all_similarities else 0
        bar = "‚ñà" * (count // 2) if count > 0 else ""
        print(f"   {range_name}: {count:3d} ({percentage:5.1f}%) {bar}")
    print()
    
    print("‚è±Ô∏è ANALYSE DES TEMPS DE R√âPONSE")
    print("-" * 40)
    if all_times:
        print(f"   Temps min: {min(all_times):.2f}s")
        print(f"   Temps max: {max(all_times):.2f}s")
        print(f"   Temps moyen: {avg_time:.2f}s")
        print(f"   √âcart-type: {np.std(all_times):.2f}s")
    else:
        print("   ‚ùå Aucun temps mesur√©")
    print()
    
    print("üéØ √âVALUATION DE LA QUALIT√â")
    print("-" * 40)
    print(f"   Scores uniques: {unique_scores}/{len(all_similarities)}")
    print(f"   Ratio de diff√©renciation: {differentiation_ratio:.3f}")
    
    if differentiation_ratio > 0.8:
        print("   ‚úÖ Excellente diff√©renciation")
    elif differentiation_ratio > 0.6:
        print("   ‚ö†Ô∏è Bonne diff√©renciation")
    else:
        print("   ‚ùå Diff√©renciation insuffisante")
    
    if avg_time < 1:
        print("   ‚úÖ Temps de r√©ponse excellent")
    elif avg_time < 5:
        print("   ‚ö†Ô∏è Temps de r√©ponse acceptable")
    else:
        print("   ‚ùå Temps de r√©ponse lent")
    print()
    
    print("üí° RECOMMANDATIONS")
    print("-" * 40)
    if avg_time > 5:
        print("   ‚ö†Ô∏è Optimiser les performances de recherche")
    if mean_similarity < 0.7:
        print("   ‚ö†Ô∏è Am√©liorer la qualit√© des embeddings")
    if differentiation_ratio < 0.8:
        print("   ‚ö†Ô∏è Am√©liorer la diff√©renciation des r√©sultats")
    
    print("   ‚úÖ Syst√®me classique fonctionnel")
    print()
    
    print("üéâ √âvaluation termin√©e !")
    
    return {
        'total_time': total_time,
        'avg_time': avg_time,
        'total_results': total_results,
        'mean_similarity': mean_similarity,
        'std_similarity': std_similarity,
        'differentiation_ratio': differentiation_ratio,
        'similarity_ranges': similarity_ranges,
        'all_results': all_results
    }

if __name__ == "__main__":
    evaluate_baseline_system_fixed()
