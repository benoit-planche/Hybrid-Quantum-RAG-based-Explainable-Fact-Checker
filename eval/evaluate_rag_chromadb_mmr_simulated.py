#!/usr/bin/env python3
"""
Ã‰valuation RAG avec ChromaDB et MMR simulÃ©
Utilise la recherche simple de ChromaDB + simulation MMR
"""

import os
import asyncio
import json
from datetime import datetime
from typing import List, Dict, Any
from dotenv import load_dotenv

# Local imports
import sys
sys.path.append('../system')
from chromadb_manager import ChromaDBManager
from ollama_utils import OllamaClient, OllamaEmbeddings
from climate_dataset import get_random_subset

# Load environment variables
load_dotenv()

class SimulatedMMREvaluator:
    """Ã‰valuateur RAG avec ChromaDB et MMR simulÃ©"""
    
    def __init__(self, embedding_model="llama2:7b", lambda_param=0.5):
        """
        Initialiser l'Ã©valuateur
        
        Args:
            embedding_model: ModÃ¨le d'embedding Ã  utiliser
            lambda_param: ParamÃ¨tre MMR (0.0 = max diversitÃ©, 1.0 = max pertinence)
        """
        self.embedding_model = embedding_model
        self.lambda_param = lambda_param
        
        # Initialiser les composants
        self.ollama_client = OllamaClient()
        self.chroma_manager = ChromaDBManager(
            persist_directory="../system/chroma_db",
            embedding_model=embedding_model
        )
        
        # Initialiser les embeddings pour la simulation MMR
        self.embeddings = OllamaEmbeddings(model=embedding_model)
        
        # Prompts pour le fact-checking
        self.fact_checking_prompt = """Tu es un expert en fact-checking sur le changement climatique.

Contexte scientifique:
{context}

Question Ã  vÃ©rifier: {question}

Analyse cette question en te basant UNIQUEMENT sur le contexte fourni.
Si le contexte ne contient pas d'information pertinente, indique-le clairement.

IMPORTANT: Limite ta rÃ©ponse Ã  500 mots maximum.

Format ta rÃ©ponse comme suit:
ANALYSE: [Ton analyse basÃ©e sur le contexte]
VERDICT: [VRAI/FAUX/MIXTE/INCONNU]
EXPLICATION: [Explication de ton verdict]
SOURCES: [RÃ©fÃ©rences au contexte utilisÃ©]
"""
    
    def cosine_similarity_safe(self, a, b):
        """Calculer la similaritÃ© cosinus de maniÃ¨re sÃ»re"""
        import numpy as np
        
        # Convertir en listes si nÃ©cessaire
        if hasattr(a, 'tolist'):
            a = a.tolist()
        if hasattr(b, 'tolist'):
            b = b.tolist()
        
        # Convertir en numpy arrays
        a = np.array(a, dtype=np.float64)
        b = np.array(b, dtype=np.float64)
        
        # Handle zero vectors
        norm_a = np.linalg.norm(a)
        norm_b = np.linalg.norm(b)
        
        if norm_a == 0 or norm_b == 0:
            return 0.0
        
        return float(np.dot(a, b) / (norm_a * norm_b))
    
    def simulate_mmr_selection(self, search_results, query_embedding, k=5):
        """Simuler MMR en post-traitant les rÃ©sultats de recherche ChromaDB"""
        import numpy as np
        
        if not search_results:
            return []
        
        # GÃ©nÃ©rer les embeddings pour les documents trouvÃ©s
        print("  ðŸ”„ Simulation MMR sur les rÃ©sultats ChromaDB...")
        document_embeddings = []
        
        for i, doc in enumerate(search_results):
            try:
                # GÃ©nÃ©rer l'embedding du document
                doc_embedding = self.embeddings.embed_query(doc['content'])
                document_embeddings.append((i, doc, doc_embedding))
            except Exception as e:
                print(f"    âš ï¸ Erreur embedding document {i}: {e}")
                continue
        
        if not document_embeddings:
            print("  âŒ Aucun embedding gÃ©nÃ©rÃ© pour MMR")
            return search_results[:k]  # Retourner les premiers rÃ©sultats
        
        # Calculer les similaritÃ©s avec la requÃªte
        similarities = []
        for idx, doc, emb in document_embeddings:
            sim = self.cosine_similarity_safe(query_embedding, emb)
            similarities.append((idx, sim, emb, doc))
        
        # Trier par similaritÃ©
        similarities.sort(key=lambda x: x[1], reverse=True)
        
        # Premier document : plus haute pertinence
        selected_indices = [similarities[0][0]]
        selected_embeddings = [similarities[0][2]]
        selected_docs = [similarities[0][3]]
        
        # Documents restants : sÃ©lection MMR
        remaining_candidates = similarities[1:]
        
        for _ in range(min(k - 1, len(remaining_candidates))):
            best_mmr_score = -1
            best_idx = -1
            best_emb = None
            best_doc = None
            
            for idx, sim, emb, doc in remaining_candidates:
                # Score de pertinence
                relevance = sim
                
                # Score de diversitÃ©
                if not selected_embeddings:
                    diversity = 1.0
                else:
                    similarities_to_selected = []
                    for selected_emb in selected_embeddings:
                        sim_to_selected = self.cosine_similarity_safe(emb, selected_emb)
                        similarities_to_selected.append(sim_to_selected)
                    diversity = 1.0 - max(similarities_to_selected)
                
                # Score MMR
                mmr_score = self.lambda_param * relevance + (1 - self.lambda_param) * diversity
                
                if mmr_score > best_mmr_score:
                    best_mmr_score = mmr_score
                    best_idx = idx
                    best_emb = emb
                    best_doc = doc
            
            if best_idx != -1:
                selected_indices.append(best_idx)
                selected_embeddings.append(best_emb)
                selected_docs.append(best_doc)
                # Retirer le candidat sÃ©lectionnÃ©
                remaining_candidates = [(idx, sim, emb, doc) for idx, sim, emb, doc in remaining_candidates if idx != best_idx]
            else:
                break
        
        return selected_docs
    
    def get_rag_response(self, question: str, n_results: int = 5) -> Dict[str, Any]:
        """
        Obtenir une rÃ©ponse RAG complÃ¨te avec MMR simulÃ©
        
        Returns:
            Dict avec 'response', 'context', 'sources', 'similarity_scores'
        """
        try:
            # 1. Utiliser la recherche simple de ChromaDB (qui fonctionne)
            # RÃ©cupÃ©rer plus de rÃ©sultats pour une meilleure Ã©valuation MMR
            search_results = self.chroma_manager.search_documents(question, n_results=n_results*4)  # 4x plus de documents
            
            if not search_results:
                print("âŒ Aucun document trouvÃ©")
                return self._error_response("Aucun document trouvÃ©")
            
            # 2. GÃ©nÃ©rer l'embedding de la requÃªte pour MMR
            query_embedding = self.embeddings.embed_query(question)
            
            # 3. Appliquer MMR simulÃ© sur les rÃ©sultats
            mmr_selected_docs = self.simulate_mmr_selection(search_results, query_embedding, k=n_results)
            
            if not mmr_selected_docs:
                # Fallback: utiliser les rÃ©sultats originaux
                mmr_selected_docs = search_results[:n_results]
            
            # 4. Extraire les informations
            context_parts = []
            sources = []
            similarity_scores = []
            
            for doc in mmr_selected_docs:
                context_parts.append(doc['content'])
                sources.append({
                    'source': doc['metadata'].get('source', 'Unknown'),
                    'similarity': doc['similarity'],
                    'distance': doc['distance'],
                    'mmr_selected': True
                })
                similarity_scores.append(doc['similarity'])
            
            context = "\n\n".join(context_parts)
            
            # 5. GÃ©nÃ©rer la rÃ©ponse avec le contexte
            prompt = self.fact_checking_prompt.format(
                context=context,
                question=question
            )
            
            response = self.ollama_client.generate(prompt, temperature=0.2)
            
            return {
                'response': response,
                'context': context,
                'sources': sources,
                'similarity_scores': similarity_scores,
                'context_length': len(context),
                'mmr_simulated': True,
                'lambda_param': self.lambda_param,
                'initial_pool_size': len(search_results),  # Ajouter cette info
                'final_selection_size': len(mmr_selected_docs)
            }
            
        except Exception as e:
            print(f"âŒ Erreur lors de la gÃ©nÃ©ration RAG: {e}")
            return self._error_response(str(e))
    
    def _error_response(self, error_msg: str) -> Dict[str, Any]:
        """CrÃ©er une rÃ©ponse d'erreur standardisÃ©e"""
        return {
            'response': f"Erreur: {error_msg}",
            'context': "",
            'sources': [],
            'similarity_scores': [],
            'context_length': 0,
            'mmr_simulated': False,
            'lambda_param': self.lambda_param
        }
    
    async def evaluate_rag_system(self, max_questions: int = 10) -> Dict[str, Any]:
        """
        Ã‰valuer le systÃ¨me RAG complet avec MMR simulÃ©
        
        Args:
            max_questions: Nombre maximum de questions Ã  Ã©valuer
            
        Returns:
            RÃ©sultats complets de l'Ã©valuation
        """
        test_cases = get_random_subset(max_questions)
        
        print(f"ðŸ§ª Ã‰valuation du RAG avec ChromaDB et MMR simulÃ©")
        print(f"ðŸ¤– ModÃ¨le d'embedding: {self.embedding_model}")
        print(f"ðŸ“Š ParamÃ¨tre MMR (Î»): {self.lambda_param}")
        print(f"ðŸ“‹ {len(test_cases)} cas de test")
        print("=" * 60)
        
        # GÃ©nÃ©rer les rÃ©ponses du modÃ¨le
        print("ðŸ”„ GÃ©nÃ©ration des rÃ©ponses RAG avec MMR simulÃ©...")
        rag_details = []
        
        for i, test_case in enumerate(test_cases):
            print(f"  [{i+1}/{len(test_cases)}] {test_case['question'][:60]}...")
            
            # Obtenir la rÃ©ponse RAG avec MMR simulÃ©
            rag_result = self.get_rag_response(test_case['question'])
            
            # Stocker les dÃ©tails pour l'analyse
            rag_details.append({
                'question': test_case['question'],
                'expected': test_case['expected_answer'],
                'actual': rag_result['response'],
                'category': test_case['category'],
                'context': rag_result['context'],
                'sources': rag_result['sources'],
                'similarity_scores': rag_result['similarity_scores'],
                'context_length': rag_result['context_length'],
                'mmr_simulated': rag_result['mmr_simulated'],
                'lambda_param': rag_result['lambda_param']
            })
        
        # Analyser les rÃ©sultats par catÃ©gorie
        category_analysis = self.analyze_by_category(rag_details)
        
        # Calculer des mÃ©triques supplÃ©mentaires
        additional_metrics = self.calculate_additional_metrics(rag_details)
        
        # Compiler tous les rÃ©sultats
        final_results = {
            'evaluation_date': datetime.now().isoformat(),
            'model': self.embedding_model,
            'lambda_param': self.lambda_param,
            'category_analysis': category_analysis,
            'additional_metrics': additional_metrics,
            'rag_details': rag_details,
            'test_cases_count': len(test_cases)
        }
        
        return final_results
    
    def analyze_by_category(self, rag_details: List[Dict]) -> Dict[str, Dict]:
        """Analyser les rÃ©sultats par catÃ©gorie"""
        categories = {}
        
        for detail in rag_details:
            category = detail['category']
            if category not in categories:
                categories[category] = {
                    'count': 0,
                    'avg_similarity': 0,
                    'avg_context_length': 0,
                    'mmr_simulated_count': 0
                }
            
            categories[category]['count'] += 1
            if detail['mmr_simulated']:
                categories[category]['mmr_simulated_count'] += 1
            if detail['similarity_scores']:
                categories[category]['avg_similarity'] += sum(detail['similarity_scores']) / len(detail['similarity_scores'])
            categories[category]['avg_context_length'] += detail['context_length']
        
        # Calculer les moyennes
        for category in categories:
            count = categories[category]['count']
            categories[category]['avg_similarity'] /= count
            categories[category]['avg_context_length'] /= count
        
        return categories
    
    def calculate_additional_metrics(self, rag_details: List[Dict]) -> Dict[str, float]:
        """Calculer des mÃ©triques supplÃ©mentaires"""
        import numpy as np
        
        # MÃ©triques globales
        similarities = []
        context_lengths = []
        mmr_simulated_count = 0
        
        for detail in rag_details:
            if detail['mmr_simulated']:
                mmr_simulated_count += 1
            if detail['similarity_scores']:
                similarities.extend(detail['similarity_scores'])
            context_lengths.append(detail['context_length'])
        
        avg_similarity = np.mean(similarities) if similarities else 0.0
        avg_context_length = np.mean(context_lengths) if context_lengths else 0.0
        mmr_simulation_rate = mmr_simulated_count / len(rag_details) if rag_details else 0.0
        
        return {
            'avg_similarity_score': avg_similarity,
            'avg_context_length': avg_context_length,
            'mmr_simulation_rate': mmr_simulation_rate,
            'total_questions': len(rag_details)
        }
    
    def print_results(self, results: Dict[str, Any]):
        """Afficher les rÃ©sultats de l'Ã©valuation"""
        print("\n" + "="*60)
        print("ðŸ“Š RÃ‰SULTATS DE L'Ã‰VALUATION MMR SIMULÃ‰ AVEC CHROMADB")
        print("="*60)
        
        # Informations gÃ©nÃ©rales
        print(f"ðŸ“… Date: {results['evaluation_date']}")
        print(f"ðŸ¤– ModÃ¨le: {results['model']}")
        print(f"ðŸ“Š ParamÃ¨tre MMR (Î»): {results['lambda_param']}")
        print(f"ðŸ“‹ Nombre de tests: {results['test_cases_count']}")
        
        # MÃ©triques supplÃ©mentaires
        print("\nðŸ“ˆ MÃ‰TRIQUES MMR SIMULÃ‰:")
        additional_metrics = results['additional_metrics']
        print(f"  SimilaritÃ© moyenne: {additional_metrics['avg_similarity_score']:.3f}")
        print(f"  Longueur contexte moyenne: {additional_metrics['avg_context_length']:.0f} caractÃ¨res")
        print(f"  Taux de simulation MMR: {additional_metrics['mmr_simulation_rate']:.1%}")
        
        # Analyse par catÃ©gorie
        print("\nðŸ“‚ ANALYSE PAR CATÃ‰GORIE:")
        category_analysis = results['category_analysis']
        for category, metrics in category_analysis.items():
            print(f"  {category}:")
            print(f"    Nombre de tests: {metrics['count']}")
            print(f"    MMR simulÃ©: {metrics['mmr_simulated_count']}/{metrics['count']}")
            print(f"    SimilaritÃ© moyenne: {metrics['avg_similarity']:.3f}")
            print(f"    Longueur contexte moyenne: {metrics['avg_context_length']:.0f} caractÃ¨res")
    
    def save_results(self, results: Dict[str, Any], filename: str = None):
        """Sauvegarder les rÃ©sultats dans un fichier JSON"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"rag_eval_mmr_simulated_lambda{self.lambda_param}_{timestamp}.json"
        
        filepath = filename  # Sauvegarder dans le rÃ©pertoire courant
        
        # Convertir les numpy arrays en listes pour la sÃ©rialisation JSON
        def convert_numpy(obj):
            import numpy as np
            if isinstance(obj, np.ndarray):
                return obj.tolist()
            elif isinstance(obj, np.integer):
                return int(obj)
            elif isinstance(obj, np.floating):
                return float(obj)
            return obj
        
        # Convertir rÃ©cursivement
        def convert_recursive(obj):
            if isinstance(obj, dict):
                return {k: convert_recursive(v) for k, v in obj.items()}
            elif isinstance(obj, list):
                return [convert_recursive(v) for v in obj]
            else:
                return convert_numpy(obj)
        
        results_converted = convert_recursive(results)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(results_converted, f, indent=2, ensure_ascii=False)
        
        print(f"ðŸ’¾ RÃ©sultats sauvegardÃ©s dans: {filepath}")
        return filepath

async def main():
    """Fonction principale pour l'Ã©valuation MMR simulÃ© avec ChromaDB"""
    print("ðŸš€ DÃ©marrage de l'Ã©valuation RAG avec ChromaDB et MMR simulÃ©")
    
    # ParamÃ¨tres d'Ã©valuation
    embedding_model = "llama2:7b"
    lambda_param = 0.5  # ParamÃ¨tre MMR (0.0 = max diversitÃ©, 1.0 = max pertinence)
    max_questions = 10  # Nombre de questions Ã  tester
    
    # CrÃ©er l'Ã©valuateur
    evaluator = SimulatedMMREvaluator(
        embedding_model=embedding_model,
        lambda_param=lambda_param
    )
    
    # Ã‰valuer le systÃ¨me
    results = await evaluator.evaluate_rag_system(max_questions=max_questions)
    
    # Afficher les rÃ©sultats
    evaluator.print_results(results)
    
    # Sauvegarder les rÃ©sultats
    evaluator.save_results(results)
    
    print("\nâœ… Ã‰valuation MMR simulÃ© avec ChromaDB terminÃ©e!")

if __name__ == "__main__":
    asyncio.run(main()) 