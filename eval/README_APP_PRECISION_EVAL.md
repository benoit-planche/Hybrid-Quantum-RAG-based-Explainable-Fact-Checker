# ğŸ“Š App.py Precision Evaluation

Ce script Ã©value la prÃ©cision de votre application Streamlit (`app.py`) en testant sa capacitÃ© Ã  fact-checker des affirmations sur le changement climatique.

## ğŸ¯ Objectif

Tester la prÃ©cision des verdicts gÃ©nÃ©rÃ©s par votre systÃ¨me de fact-checking RAG en comparant les verdicts prÃ©dits avec les verdicts attendus pour un ensemble de test d'affirmations sur le climat.

## ğŸ“‹ FonctionnalitÃ©s

- **Test de 10 affirmations** couvrant 8 catÃ©gories diffÃ©rentes
- **Ã‰valuation des verdicts** : TRUE ou FALSE seulement
- **Analyse par catÃ©gorie** pour identifier les forces et faiblesses
- **MÃ©triques dÃ©taillÃ©es** : prÃ©cision globale et par catÃ©gorie
- **Sauvegarde des rÃ©sultats** au format JSON

## ğŸš€ Utilisation

### MÃ©thode 1 : Script simple

```bash
cd eval
python run_app_precision_eval.py
```

### MÃ©thode 2 : Script avancÃ© avec paramÃ¨tres

```bash
cd eval
python evaluate_app_precision.py --lambda 0.7 --output my_results.json
```

### ParamÃ¨tres disponibles

- `--lambda` : ParamÃ¨tre MMR (0.0-1.0, dÃ©faut: 0.5)
- `--output` : Nom du fichier de sortie (dÃ©faut: auto-gÃ©nÃ©rÃ©)

## ğŸ“Š MÃ©triques Ã©valuÃ©es

### PrÃ©cision des verdicts

- **Verdict correct** : Le verdict prÃ©dit correspond au verdict attendu
- **PrÃ©cision globale** : Pourcentage de verdicts corrects sur l'ensemble des tests
- **PrÃ©cision par catÃ©gorie** : Performance sur chaque type d'affirmation

### CatÃ©gories testÃ©es

1. **controversies** : Controverses sur les organisations scientifiques
2. **pause** : Pause dans le rÃ©chauffement climatique
3. **consensus** : Consensus scientifique
4. **natural_cycles** : Cycles naturels et cycles solaires
5. **models** : FiabilitÃ© des modÃ¨les climatiques
6. **ice** : Perte de glace (Arctique/Antarctique)
7. **co2** : Effets du CO2 sur les plantes
8. **sea_level** : Ã‰lÃ©vation du niveau de la mer

## ğŸ“ˆ InterprÃ©tation des rÃ©sultats

### Scores de prÃ©cision

- **90-100%** : Excellent - Le systÃ¨me fonctionne trÃ¨s bien
- **70-89%** : Bon - Quelques amÃ©liorations possibles
- **50-69%** : Moyen - AmÃ©liorations nÃ©cessaires
- **<50%** : Faible - ProblÃ¨mes majeurs Ã  rÃ©soudre

### Analyse des erreurs

- **Faux positifs** : Verdict TRUE pour une affirmation fausse
- **Faux nÃ©gatifs** : Verdict FALSE pour une affirmation vraie
- **Verdicts FALSE par dÃ©faut** : Manque de preuves pertinentes

## ğŸ”§ PrÃ©requis

1. **Cassandra Vector Store** initialisÃ© avec des documents
2. **Ollama** en cours d'exÃ©cution avec le modÃ¨le `llama2:7b`
3. **Documents indexÃ©s** dans la base vectorielle

### VÃ©rification des prÃ©requis

```bash
# VÃ©rifier qu'Ollama fonctionne
ollama list

# VÃ©rifier que Cassandra est accessible
python -c "from system.cassandra_manager import create_cassandra_manager; cm = create_cassandra_manager(); print(cm.get_collection_info())"
```

## ğŸ“ Structure des rÃ©sultats

Le script gÃ©nÃ¨re un fichier JSON avec :

```json
{
  "evaluation_time": 120.5,
  "total_tests": 10,
  "correct_verdicts": 7,
  "accuracy": 0.7,
  "category_accuracy": {
    "consensus": {"correct": 1, "total": 1, "accuracy": 1.0},
    "ice": {"correct": 1, "total": 2, "accuracy": 0.5}
  },
  "lambda_param": 0.5,
  "test_results": [...],
  "model_info": {...}
}
```

## ğŸ› ï¸ DÃ©pannage

### Erreur : "Cassandra Vector Store non initialisÃ©"

```bash
# VÃ©rifier la configuration Cassandra
python -c "from system.cassandra_manager import create_cassandra_manager; cm = create_cassandra_manager()"
```

### Erreur : "Aucun document dans la base vectorielle"

```bash
# Indexer des documents
cd system
python index_documents.py
```

### Erreur : "Ollama non accessible"

```bash
# DÃ©marrer Ollama
ollama serve
```

## ğŸ“ Exemple de sortie

```
ğŸš€ Starting app.py precision evaluation...

============================================================
Test Case 1/10: controversies
============================================================
ğŸ” Fact-checking: Is the IPCC a political body?
ğŸ“š Retrieving relevant information...
ğŸ” RequÃªtes de recherche gÃ©nÃ©rÃ©es: IPCC political body, IPCC scientific organization, IPCC intergovernmental panel
ğŸ“Š Documents rÃ©cupÃ©rÃ©s: 3/5 pertinents
ğŸ§  Analyzing claim against evidence...
ğŸ“ Generating summary...
âœ… CORRECT | Expected: FALSE | Predicted: FALSE
Documents retrieved: 3

============================================================
Test Case 2/10: pause
============================================================
...

ğŸ“Š EVALUATION SUMMARY
============================================================
â±ï¸  Total evaluation time: 45.23 seconds
ğŸ“ Total test cases: 10
âœ… Correct verdicts: 7
ğŸ¯ Overall accuracy: 70.00%
ğŸ”§ Lambda parameter: 0.5

ğŸ“ˆ Accuracy by category:
  controversies: 100.00% (1/1)
  pause: 100.00% (1/1)
  consensus: 100.00% (1/1)
  natural_cycles: 50.00% (1/2)
  models: 100.00% (1/1)
  ice: 50.00% (1/2)
  co2: 100.00% (1/1)
  sea_level: 100.00% (1/1)

ğŸ¤– Model information:
  Embedding model: llama2:7b
  LLM model: llama2:7b (Ollama)
  Categories tested: controversies, pause, consensus, natural_cycles, models, ice, co2, sea_level

ğŸ’¾ Results saved to: app_precision_eval_20241216_143022.json

ğŸ‰ Evaluation completed successfully!
```

## ğŸ”„ AmÃ©lioration continue

1. **Analyser les erreurs** pour identifier les patterns
2. **Ajuster les prompts** selon les rÃ©sultats
3. **Optimiser les paramÃ¨tres MMR** (lambda)
4. **AmÃ©liorer la qualitÃ© des embeddings**
5. **Ajouter plus de documents** Ã  la base de connaissances

## ğŸ“ Support

En cas de problÃ¨me, vÃ©rifiez :

1. Les logs d'erreur dans la console
2. La configuration de Cassandra
3. L'Ã©tat d'Ollama
4. La prÃ©sence de documents indexÃ©s
