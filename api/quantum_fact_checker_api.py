#!/usr/bin/env python3
"""
API pour le syst√®me de fact-checking quantique
Remplace l'interface Streamlit par une API REST
"""

import os
import sys
import time
import json
import logging
import numpy as np
from datetime import datetime
from typing import Dict, List, Any, Optional
from pydantic import BaseModel, Field

# Configuration des logs vers un fichier
log_file = "api_quantum_fact_checker.log"
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_file, mode='w'),
        logging.StreamHandler()  # Garder aussi la console
    ]
)
logger = logging.getLogger(__name__)

# Ajouter le dossier system au PYTHONPATH
current_dir = os.path.dirname(os.path.abspath(__file__))
system_dir = os.path.join(os.path.dirname(current_dir), 'system')
quantum_dir = os.path.join(os.path.dirname(current_dir), 'src', 'quantum')
sys.path.insert(0, system_dir)
sys.path.insert(0, quantum_dir)

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import uvicorn

# Imports du syst√®me quantique
from quantum_search import retrieve_top_k
from cassandra_manager import create_cassandra_manager
from ollama_utils import OllamaClient, format_prompt
from performance_metrics import (
    start_performance_session, 
    get_performance_summary, 
    save_performance_metrics,
    time_operation_context,
    log_llm_operation,
    log_database_operation
)

# Configuration de l'API
app = FastAPI(
    title="Quantum Fact-Checker API",
    description="API pour v√©rifier la v√©racit√© des messages li√©s au climat",
    version="1.0.0"
)

# Configuration CORS pour permettre l'acc√®s depuis les applications web
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # En production, sp√©cifiez les domaines autoris√©s
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Mod√®les Pydantic pour l'API
class FactCheckRequest(BaseModel):
    message: str = Field(..., description="Message √† v√©rifier", min_length=1, max_length=1000)
    user_id: Optional[str] = Field(None, description="ID de l'utilisateur (optionnel)")
    context: Optional[str] = Field(None, description="Contexte suppl√©mentaire (optionnel)")
    language: str = Field("en", description="Langue du message (d√©faut: en)")

class FactCheckResponse(BaseModel):
    message_id: str
    certainty_score: float = Field(..., ge=0.0, le=1.0, description="Score de certitude (0-1)")
    verdict: str = Field(..., description="Verdict: TRUE, FALSE, MIXED, UNVERIFIABLE")
    explanation: str = Field(..., description="Explication d√©taill√©e du verdict")
    confidence_level: str = Field(..., description="Niveau de confiance: HIGH, MEDIUM, LOW")
    sources_used: List[str] = Field(..., description="Sources utilis√©es pour la v√©rification")
    processing_time: float = Field(..., description="Temps de traitement en secondes")
    timestamp: str = Field(..., description="Timestamp de la v√©rification")

class HealthResponse(BaseModel):
    status: str
    quantum_system: str
    cassandra_status: str
    ollama_status: str
    timestamp: str

class QuantumFactCheckerAPI:
    """Classe principale pour g√©rer l'API de fact-checking quantique"""
    
    def __init__(self):
        """Initialiser l'API"""
        self.db_folder = "../src/quantum/quantum_db_8qubits/"
        self.n_qubits = 8
        self.k_results = 10
        
        # Initialiser les composants
        self._initialize_components()
        
        # Template pour l'analyse LLM - VERSION STRICTE ET DIRECTIVE
        self.analysis_prompt_template = """
You are a RIGOROUS and DECISIVE fact-checker. Your job is to verify the following claim using ONLY the provided evidence. 
You MUST take a clear position and be willing to CONTRADICT the claim if the evidence doesn't support it.
- Be EXTRA CRITICAL of claims that seem too good to be true
- If a claim sounds like climate denial talking points, be especially skeptical
- Look for evidence that DIRECTLY contradicts the claim
- Remember: absence of evidence is not evidence of absence

CLAIM: {claim}

EVIDENCE (from chunks):
{retrieved_docs}

CRITICAL INSTRUCTIONS - READ CAREFULLY:
1. You MUST be DECISIVE. No hedging, no "maybe", no uncertainty.
2. If the evidence DIRECTLY supports the claim, say TRUE.
3. If the evidence CONTRADICTS the claim, say FALSE.
4. If the evidence is UNRELATED or INSUFFICIENT, say UNVERIFIABLE.
5. You MUST quote SPECIFIC text from the evidence to justify your verdict.
6. If the evidence talks about OTHER regions (like Karakoram, Himalayas) but NOT Antarctica, this is NOT evidence for the claim.
7. Be willing to say FALSE if the evidence doesn't specifically support the claim about Antarctica.

ANTARCTICA-SPECIFIC ANALYSIS:
- Look for DIRECT statements about Antarctica ice loss/gain
- If evidence mentions other glaciers (Karakoram, Himalayas), this is NOT about Antarctica
- If evidence is about general climate change but not Antarctica ice, this is NOT sufficient
- The claim is SPECIFICALLY about Antarctica gaining ice due to climate change

Format your response EXACTLY as follows:
VERDICT: [TRUE/FALSE/UNVERIFIABLE]
EXPLANATION: [Your decisive reasoning with specific quotes from the evidence. Be direct and clear about why you chose this verdict.]
"""
    
    def _initialize_components(self):
        """Initialiser tous les composants du syst√®me"""
        try:
            print("üîå Initialisation de l'API Quantum Fact-Checker...")
            
            # Connexion Cassandra
            print("  üìä Connexion √† Cassandra...")
            self.cassandra_manager = create_cassandra_manager(
                table_name="fact_checker_docs", 
                keyspace="fact_checker_keyspace"
            )
            # Utiliser la session Cassandra du cassandra_manager
            self.cassandra_session = self.cassandra_manager.session
            print("  ‚úÖ Session Cassandra initialis√©e")
            
            # Client Ollama
            print("  ü§ñ Initialisation du client Ollama...")
            self.ollama_client = OllamaClient()
            
            # Test de connexion
            print("  ‚úÖ Test de connexion...")
            test_response = self.ollama_client.generate("Test", max_tokens=5)
            print(f"  ‚úÖ Ollama OK: {len(test_response)} caract√®res g√©n√©r√©s")
            
            print("‚úÖ API initialis√©e avec succ√®s!")
            
        except Exception as e:
            print(f"‚ùå Erreur d'initialisation: {e}")
            raise
    
    def get_chunk_info(self, chunk_id: str) -> tuple[str, str]:
        """R√©cup√©rer les informations d'un chunk depuis Cassandra"""
        try:
            # Utiliser la session Cassandra existante si disponible
            if hasattr(self, 'cassandra_session'):
                session = self.cassandra_session
            else:
                # Cr√©er une nouvelle session si n√©cessaire
                from cassandra.cluster import Cluster
                cluster = Cluster(['localhost'], port=9042)
                session = cluster.connect()
                self.cassandra_session = session
            
            # Le chunk_id est maintenant au format "doc_157" (apr√®s nettoyage dans quantum_search.py)
            # Essayer d'abord avec partition_id = "None"
            partition_id = "None"
            row_id = chunk_id  # Utiliser "doc_157" comme row_id
            
            query = "SELECT body_blob, metadata_s FROM fact_checker_keyspace.fact_checker_docs WHERE partition_id=%s AND row_id=%s;"
            row = session.execute(query, (partition_id, row_id)).one()
            
            # Si pas trouv√©, essayer de r√©cup√©rer directement par row_id
            if not row or not row.body_blob:
                query_fallback = "SELECT body_blob, metadata_s FROM fact_checker_keyspace.fact_checker_docs WHERE row_id=%s LIMIT 1 ALLOW FILTERING;"
                row = session.execute(query_fallback, (row_id,)).one()
            
            if row and row.body_blob:
                chunk_text = row.body_blob
                pdf_name = row.metadata_s.get('source', '[PDF inconnu]') if row.metadata_s else '[PDF inconnu]'
                return chunk_text, pdf_name
            else:
                return "[Texte non trouv√©]", "[PDF inconnu]"
                
        except Exception:
            return "[Erreur de r√©cup√©ration]", "[Erreur]"
    
    def generate_llm_response(self, claim: str, chunk_ids: List[str]) -> tuple[str, str]:
        """G√©n√©rer la r√©ponse LLM pour l'analyse"""
        try:
            # Pr√©parer les documents (m√™me format que l'app Streamlit)
            docs = []
            for chunk_id in chunk_ids:
                chunk_text, pdf_name = self.get_chunk_info(chunk_id)
                # Prendre plus de contexte pour une meilleure analyse (comme l'app Streamlit)
                excerpt = chunk_text[:1500] + ("..." if len(chunk_text) > 1500 else "")
                docs.append(f"[Source PDF: {pdf_name}]\n[Chunk ID: {chunk_id}]\n{excerpt}")
            
            retrieved_docs = "\n\n".join(docs)
            
            # Formater le prompt
            prompt = format_prompt(
                self.analysis_prompt_template, 
                claim=claim, 
                retrieved_docs=retrieved_docs
            )
            
            # G√©n√©rer la r√©ponse avec temp√©rature tr√®s basse pour √™tre d√©cisif
            response = self.ollama_client.generate(
                prompt, 
                temperature=0.01  # Temp√©rature tr√®s basse pour des r√©ponses d√©cisives et coh√©rentes
            )
            
            return prompt, response
            
        except Exception as e:
            return "", f"Erreur lors de l'analyse: {str(e)}"
    
    def parse_llm_response(self, response: str) -> Dict[str, Any]:
        """Parser la r√©ponse LLM pour extraire les informations (m√™me format que l'app Streamlit)"""
        try:
            lines = response.strip().split('\n')
            result = {
                'verdict': 'UNVERIFIABLE',
                'confidence': 'MEDIUM',  # Valeur par d√©faut
                'explanation': 'Impossible de parser la r√©ponse LLM',
                'sources': []
            }
            
            current_section = None
            explanation_lines = []
            
            for line in lines:
                line = line.strip()
                if line.startswith('VERDICT:'):
                    verdict = line.replace('VERDICT:', '').strip()
                    if verdict in ['TRUE', 'FALSE', 'UNVERIFIABLE']:
                        result['verdict'] = verdict
                    current_section = 'verdict'
                elif line.startswith('EXPLANATION:'):
                    current_section = 'explanation'
                    # Commencer √† collecter l'explication
                    explanation_text = line.replace('EXPLANATION:', '').strip()
                    if explanation_text:
                        explanation_lines.append(explanation_text)
                elif current_section == 'explanation' and line:
                    # Continuer √† collecter l'explication
                    explanation_lines.append(line)
            
            # Joindre toutes les lignes d'explication
            if explanation_lines:
                result['explanation'] = '\n'.join(explanation_lines)
            else:
                # Si pas d'explication trouv√©e, prendre tout apr√®s VERDICT:
                response_parts = response.split('VERDICT:')
                if len(response_parts) > 1:
                    explanation_part = response_parts[1].split('\n', 1)
                    if len(explanation_part) > 1:
                        result['explanation'] = explanation_part[1].strip()
                    else:
                        result['explanation'] = "Pas d'explication fournie"
            
            # D√©terminer la confiance bas√©e sur le verdict et l'explication
            if result['verdict'] in ['TRUE', 'FALSE'] and len(result['explanation']) > 100:
                result['confidence'] = 'HIGH'
            elif result['verdict'] in ['TRUE', 'FALSE']:
                result['confidence'] = 'MEDIUM'
            else:
                result['confidence'] = 'LOW'
            
            return result
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur parsing LLM: {e}")
            return {
                'verdict': 'UNVERIFIABLE',
                'confidence': 'LOW',
                'explanation': f'Erreur de parsing: {str(e)}',
                'sources': []
            }
    
    def calculate_certainty_score(self, similarity_scores: List[float], llm_result: Dict[str, Any]) -> float:
        """Calculer le score de certitude bas√© sur les similarit√©s et le verdict LLM"""
        try:
            if not similarity_scores:
                return 0.0
            
            # Score bas√© sur la similarit√© moyenne
            avg_similarity = np.mean(similarity_scores)
            max_similarity = max(similarity_scores)
            
            # Score bas√© sur le verdict LLM
            verdict_scores = {
                'TRUE': 0.8,
                'FALSE': 0.8,
                'UNVERIFIABLE': 0.2
            }
            
            confidence_multipliers = {
                'HIGH': 1.0,
                'MEDIUM': 0.8,
                'LOW': 0.6
            }
            
            # Calculer le score final
            similarity_score = (avg_similarity * 0.6 + max_similarity * 0.4)
            verdict_score = verdict_scores.get(llm_result['verdict'], 0.5)
            confidence_multiplier = confidence_multipliers.get(llm_result['confidence'], 0.7)
            
            # Combiner les scores
            final_score = (similarity_score * 0.4 + verdict_score * 0.6) * confidence_multiplier
            
            # Normaliser entre 0 et 1
            return min(1.0, max(0.0, final_score))
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erreur calcul score: {e}")
            return 0.5
    
    async def fact_check_message(self, request: FactCheckRequest) -> FactCheckResponse:
        """V√©rifier la v√©racit√© d'un message"""
        start_time = time.time()
        message_id = f"msg_{int(time.time() * 1000)}"
        
        try:
            # D√©marrer la session de performance
            start_time = time.time()
            
            # Recherche quantique
            quantum_search_start = time.time()
            with time_operation_context("quantum_search"):
                results = retrieve_top_k(
                    request.message,
                    self.db_folder,
                    k=self.k_results,
                    n_qubits=self.n_qubits,
                    cassandra_manager=self.cassandra_manager
                )
            quantum_search_time = time.time() - quantum_search_start
            
            # Analyser les r√©sultats
            chunk_ids = [chunk_id for score, qasm_path, chunk_id in results]
            similarity_scores = [score for score, qasm_path, chunk_id in results]
            
            # G√©n√©rer la r√©ponse LLM
            llm_start = time.time()
            with time_operation_context("llm_analysis"):
                prompt, llm_response = self.generate_llm_response(request.message, chunk_ids)
            llm_time = time.time() - llm_start
            
            # Parser la r√©ponse LLM
            parsing_start = time.time()
            llm_result = self.parse_llm_response(llm_response)
            parsing_time = time.time() - parsing_start
            
            # Calculer le score de certitude
            score_start = time.time()
            certainty_score = self.calculate_certainty_score(similarity_scores, llm_result)
            score_time = time.time() - score_start
            
            # R√©cup√©rer les sources utilis√©es
            sources_start = time.time()
            sources_used = []
            for chunk_id in chunk_ids[:5]:  # Limiter aux 5 premi√®res sources
                _, pdf_name = self.get_chunk_info(chunk_id)
                if pdf_name not in sources_used:
                    sources_used.append(pdf_name)
            sources_time = time.time() - sources_start
            
            # Log des m√©triques de performance
            print(f"\n‚è±Ô∏è M√âTRIQUES DE PERFORMANCE:")
            print(f"{'='*80}")
            print(f"üîç Recherche quantique: {quantum_search_time:.3f}s")
            print(f"ü§ñ G√©n√©ration LLM: {llm_time:.3f}s")
            print(f"üìù Parsing LLM: {parsing_time:.3f}s")
            print(f"üìä Calcul score: {score_time:.3f}s")
            print(f"üìö R√©cup√©ration sources: {sources_time:.3f}s")
            print(f"‚è±Ô∏è Temps total: {time.time() - start_time:.3f}s")
            print(f"{'='*80}")
            
            # Log d√©taill√© des chunks r√©cup√©r√©s (top 10)
            print(f"\nüîç TOP 10 CHUNKS R√âCUP√âR√âS PAR LE CIRCUIT QUANTIQUE:")
            print(f"{'='*80}")
            for i, (score, _qasm_path, chunk_id) in enumerate(results[:10]):
                chunk_text, pdf_name = self.get_chunk_info(chunk_id)
                excerpt = chunk_text[:200] + ("..." if len(chunk_text) > 200 else "")
                print(f"{i+1:2d}. Chunk: {chunk_id}")
                print(f"    üìä Similarit√©: {score:.4f}")
                print(f"    üìÑ PDF: {pdf_name}")
                print(f"    üìù Extrait: {excerpt}")
                print(f"    {'-'*60}")
            
            # Log de la r√©ponse brute du LLM
            print(f"\nü§ñ R√âPONSE BRUTE DU LLM (OLLAMA):")
            print(f"{'='*80}")
            print(f"üìù Prompt envoy√©:")
            print(f"{'='*40}")
            print(prompt)
            print(f"{'='*40}")
            print(f"üì§ R√©ponse brute re√ßue:")
            print(f"{'='*40}")
            print(llm_response)
            print(f"{'='*40}")
            
            processing_time = time.time() - start_time
            
            return FactCheckResponse(
                message_id=message_id,
                certainty_score=certainty_score,
                verdict=llm_result['verdict'],
                explanation=llm_result['explanation'],
                confidence_level=llm_result['confidence'],
                sources_used=sources_used,
                processing_time=processing_time,
                timestamp=datetime.now().isoformat(),
            )
            
        except Exception as e:
            print(f"‚ùå Erreur fact-checking: {e}")
            processing_time = time.time() - start_time
            
            return FactCheckResponse(
                message_id=message_id,
                certainty_score=0.0,
                verdict="UNVERIFIABLE",
                explanation=f"Erreur lors de la v√©rification: {str(e)}",
                confidence_level="LOW",
                sources_used=[],
                processing_time=processing_time,
                timestamp=datetime.now().isoformat()
            )

# Instance globale de l'API
api_instance = None

@app.on_event("startup")
async def startup_event():
    """√âv√©nement de d√©marrage de l'API"""
    global api_instance
    try:
        api_instance = QuantumFactCheckerAPI()
        print("üöÄ API Quantum Fact-Checker d√©marr√©e avec succ√®s!")
    except Exception as e:
        print(f"‚ùå Erreur de d√©marrage: {e}")
        raise

@app.get("/", response_model=Dict[str, str])
async def root():
    """Endpoint racine"""
    return {
        "message": "Quantum Fact-Checker API",
        "version": "1.0.0",
        "status": "running"
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """V√©rification de l'√©tat de sant√© de l'API"""
    try:
        # Test des composants
        quantum_status = "OK"
        cassandra_status = "OK"
        ollama_status = "OK"
        
        # Test Ollama
        try:
            test_response = api_instance.ollama_client.generate("Test", max_tokens=5)
            ollama_status = "OK"
        except:
            ollama_status = "ERROR"
        
        # Test Cassandra
        try:
            test_query = f"SELECT COUNT(*) FROM {api_instance.cassandra_manager.keyspace}.{api_instance.cassandra_manager.table_name} LIMIT 1"
            api_instance.cassandra_manager.session.execute(test_query)
            cassandra_status = "OK"
        except:
            cassandra_status = "ERROR"
        
        return HealthResponse(
            status="healthy" if all(s == "OK" for s in [quantum_status, cassandra_status, ollama_status]) else "degraded",
            quantum_system=quantum_status,
            cassandra_status=cassandra_status,
            ollama_status=ollama_status,
            timestamp=datetime.now().isoformat()
        )
        
    except Exception as e:
        return HealthResponse(
            status="unhealthy",
            quantum_system="ERROR",
            cassandra_status="ERROR",
            ollama_status="ERROR",
            timestamp=datetime.now().isoformat()
        )

@app.post("/fact-check", response_model=FactCheckResponse)
async def fact_check(request: FactCheckRequest):
    """V√©rifier la v√©racit√© d'un message"""
    if not api_instance:
        raise HTTPException(status_code=503, detail="API non initialis√©e")
    
    try:
        result = await api_instance.fact_check_message(request)
        return result
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur lors de la v√©rification: {str(e)}")

@app.post("/fact-check/batch", response_model=List[FactCheckResponse])
async def fact_check_batch(requests: List[FactCheckRequest]):
    """V√©rifier plusieurs messages en lot"""
    if not api_instance:
        raise HTTPException(status_code=503, detail="API non initialis√©e")
    
    try:
        results = []
        for request in requests:
            result = await api_instance.fact_check_message(request)
            results.append(result)
        return results
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur lors de la v√©rification en lot: {str(e)}")

@app.get("/stats")
async def get_stats():
    """Obtenir les statistiques de l'API"""
    try:
        performance_summary = get_performance_summary()
        return {
            "timestamp": datetime.now().isoformat(),
            "performance_metrics": performance_summary,
            "system_info": {
                "n_qubits": api_instance.n_qubits,
                "db_folder": api_instance.db_folder,
                "k_results": api_instance.k_results
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Erreur lors de la r√©cup√©ration des stats: {str(e)}")

if __name__ == "__main__":
    uvicorn.run(
        "quantum_fact_checker_api:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
