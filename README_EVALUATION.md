# üß™ Guide d'√©valuation du fact-checker avec DeepEval

## üìã Vue d'ensemble

Ce guide vous explique comment √©valuer votre syst√®me de fact-checking sur le changement climatique avec DeepEval.

## üéØ Objectifs de l'√©valuation

1. **Comparer les performances** RAG vs Direct
2. **√âvaluer la pr√©cision** des r√©ponses
3. **Mesurer la pertinence** du contexte utilis√©
4. **Tester la fid√©lit√©** aux sources

## üìä Dataset d'√©valuation

Le dataset contient **100+ questions** r√©parties en cat√©gories :

- **Consensus scientifique** (10 questions)
- **Temp√©ratures** (15 questions)
- **CO2 et gaz √† effet de serre** (15 questions)
- **Mod√®les climatiques** (10 questions)
- **Pause du r√©chauffement** (10 questions)
- **Cycles naturels** (15 questions)
- **Glace et glaciers** (10 questions)
- **Niveau de la mer** (10 questions)
- **√âv√©nements extr√™mes** (10 questions)
- **Oc√©ans** (10 questions)
- **Controverses** (10 questions)

## üöÄ Installation

```bash
# Installer DeepEval et d√©pendances
pip install -r requirements_eval.txt

# V√©rifier l'installation
python -c "import deepeval; print('DeepEval install√© avec succ√®s')"
```

## üîß Configuration

Assurez-vous que votre fichier `.env` contient :

```env
# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2:7b

# Pinecone Configuration
PINECONE_API_KEY=votre_cle_api_pinecone
PINECONE_INDEX_NAME=fact-checker-index
```

## üß™ Ex√©cution des √©valuations

### 1. √âvaluation compl√®te (recommand√©e)

```bash
python evaluate_with_dataset.py
```

Cette √©valuation :

- Teste **30 questions** avec RAG
- Teste **30 questions** sans RAG
- Compare les performances
- Sauvegarde les r√©sultats d√©taill√©s

### 2. √âvaluation par cat√©gorie

```python
from climate_dataset import get_dataset_by_category

# √âvaluer seulement les questions sur le consensus
consensus_questions = get_dataset_by_category("consensus")
```

### 3. √âvaluation personnalis√©e

```python
from evaluate_with_dataset import ComprehensiveFactCheckerEvaluator

evaluator = ComprehensiveFactCheckerEvaluator(use_rag=True)
results = await evaluator.evaluate_model(max_questions=20)
```

## üìà M√©triques √©valu√©es

### AnswerRelevancy

- **Objectif** : La r√©ponse est-elle pertinente √† la question ?
- **Seuil** : 0.7
- **Interpr√©tation** : Score > 0.8 = Excellent

### ContextRelevancy

- **Objectif** : Le contexte utilis√© est-il pertinent ?
- **Seuil** : 0.7
- **Interpr√©tation** : Score > 0.8 = Excellent

### Faithfulness

- **Objectif** : La r√©ponse est-elle fid√®le au contexte fourni ?
- **Seuil** : 0.7
- **Interpr√©tation** : Score > 0.8 = Excellent

## üìä Interpr√©tation des r√©sultats

### Scores par m√©trique

- **0.8-1.0** : Excellent
- **0.7-0.8** : Bon
- **0.6-0.7** : Acceptable
- **< 0.6** : √Ä am√©liorer

### Comparaison RAG vs Direct

- **RAG > Direct** : Le syst√®me RAG am√©liore les performances
- **RAG ‚âà Direct** : Le RAG n'apporte pas d'am√©lioration significative
- **RAG < Direct** : Le RAG d√©grade les performances

## üìÅ Fichiers de sortie

### comprehensive_evaluation_results.json

Contient :

- M√©triques d√©taill√©es pour RAG et Direct
- Toutes les questions test√©es
- R√©ponses attendues vs obtenues
- Analyse par cat√©gorie

### Structure des r√©sultats

```json
{
  "rag_results": {
    "metrics": {
      "AnswerRelevancy": 0.85,
      "ContextRelevancy": 0.78,
      "Faithfulness": 0.82
    },
    "test_cases": [...]
  },
  "direct_results": {
    "metrics": {...},
    "test_cases": [...]
  }
}
```

## üîç Analyse des r√©sultats

### 1. Performance globale

```python
import json

with open("comprehensive_evaluation_results.json", "r") as f:
    results = json.load(f)

rag_score = results["rag_results"]["metrics"]["AnswerRelevancy"]
direct_score = results["direct_results"]["metrics"]["AnswerRelevancy"]

print(f"Am√©lioration RAG: {rag_score - direct_score:.3f}")
```

### 2. Analyse par cat√©gorie

```python
# Analyser les performances par cat√©gorie
categories = {}
for test_case in results["rag_results"]["test_cases"]:
    cat = test_case["category"]
    if cat not in categories:
        categories[cat] = []
    categories[cat].append(test_case)

for cat, cases in categories.items():
    print(f"{cat}: {len(cases)} questions")
```

## üö® D√©pannage

### Erreur : "DeepEval not found"

```bash
pip install deepeval
```

### Erreur : "Pinecone connection failed"

- V√©rifiez votre API key
- V√©rifiez que l'index existe
- Testez la connexion : `python test_pinecone_serverless.py`

### Erreur : "Ollama not responding"

- V√©rifiez qu'Ollama est d√©marr√© : `ollama serve`
- Testez la connexion : `python test_ollama_connection.py`

## üìà Optimisation

### Am√©liorer les scores

1. **Ajuster les prompts** pour plus de pr√©cision
2. **Optimiser la recherche** dans Pinecone
3. **Am√©liorer le chunking** des documents
4. **Utiliser un mod√®le plus performant**

### Ajouter des m√©triques

```python
from deepeval.metrics import ContextRecall

metrics = [
    AnswerRelevancy(threshold=0.7),
    ContextRelevancy(threshold=0.7),
    Faithfulness(threshold=0.7),
    ContextRecall(threshold=0.7)  # Nouvelle m√©trique
]
```

## üéØ Prochaines √©tapes

1. **Ex√©cuter l'√©valuation** d√®s que votre script de chargement se termine
2. **Analyser les r√©sultats** par cat√©gorie
3. **Identifier les points d'am√©lioration**
4. **It√©rer** sur le syst√®me

## üìû Support

Si vous rencontrez des probl√®mes :

1. V√©rifiez les logs d'erreur
2. Testez chaque composant individuellement
3. Consultez la documentation DeepEval
4. V√©rifiez la configuration Ollama et Pinecone
